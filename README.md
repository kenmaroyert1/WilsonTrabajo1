# WilsonTrabajo1 - Pipeline ETL para AnÃ¡lisis de COVID-19

> ðŸ“Š **Â¿Buscas informaciÃ³n sobre el dataset y las visualizaciones?**  
> â†’ Ver **[DATASET_INFO.md](DATASET_INFO.md)** - InformaciÃ³n del dataset, 6 casos de uso y 11 visualizaciones explicadas detalladamente

---

## ðŸš€ Inicio RÃ¡pido

### Requisitos Previos
- Python 3.7 o superior
- Dataset: `IntegratedData.csv` (colocar en la raÃ­z del proyecto)

### InstalaciÃ³n en 3 Pasos

```bash
# 1. Clonar el repositorio
git clone https://github.com/kenmaroyert1/WilsonTrabajo1.git
cd WilsonTrabajo1

# 2. Instalar dependencias
pip install -r requirements.txt

# 3. Ejecutar el pipeline completo
python pipeline.py
```

### âœ… Resultado
DespuÃ©s de ejecutar `pipeline.py`, obtendrÃ¡s:
- âœ”ï¸ Datos limpios: `Output/IntegratedData_cleaned.csv`
- âœ”ï¸ Datos transformados: `Output/IntegratedData_transformed.csv`
- âœ”ï¸ 11 grÃ¡ficas profesionales en: `Output/figures/`
- âœ”ï¸ Agregaciones: `Output/agregado_nacional.csv`, `top_estados.csv`, `top_condados.csv`

---

## ðŸ“– Sobre Este Proyecto

Este proyecto implementa un **pipeline ETL completo** para anÃ¡lisis de datos de COVID-19, procesando mÃ¡s de 935,000 registros con informaciÃ³n epidemiolÃ³gica y de movilidad de Estados Unidos.

**DocumentaciÃ³n adicional:**
- ðŸ“Š **[DATASET_INFO.md](DATASET_INFO.md)** - Para quÃ© sirve el dataset, visualizaciones y casos de uso

---

## ðŸ”§ Arquitectura del Pipeline ETL

El proyecto sigue una arquitectura modular de 5 etapas:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   EXTRACT   â”‚ --> â”‚    CLEAN    â”‚ --> â”‚  TRANSFORM  â”‚ --> â”‚    LOAD     â”‚ --> â”‚  VISUALIZE  â”‚
â”‚   Lectura   â”‚     â”‚  Limpieza   â”‚     â”‚  AnÃ¡lisis   â”‚     â”‚  Guardado   â”‚     â”‚  GrÃ¡ficas   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Resumen de Etapas

1. **Extract** - Lectura eficiente de CSV (77MB+) con procesamiento por chunks
2. **Clean** - NormalizaciÃ³n de columnas, eliminaciÃ³n de duplicados
3. **Transform** - CÃ¡lculo de mÃ©tricas derivadas y agregaciones
4. **Load** - Guardado en mÃºltiples formatos con backups
5. **Visualize** - 11 grÃ¡ficas profesionales en espaÃ±ol (300 DPI)

---

## ðŸ“¦ ExplicaciÃ³n del CÃ³digo - MÃ³dulos

### âš™ï¸ Config/Config.py - ConfiguraciÃ³n Centralizada

**Â¿QuÃ© hace?**
Almacena TODAS las configuraciones del proyecto en un solo lugar.

**Contiene:**
```python
# Rutas de directorios
PROJECT_ROOT = Path(__file__).parent.parent
DATA_DIR = PROJECT_ROOT / "Data"
OUTPUT_DIR = PROJECT_ROOT / "Output"
FIGURES_DIR = OUTPUT_DIR / "figures"

# ParÃ¡metros de procesamiento
CHUNK_SIZE = 100000  # Filas por chunk
DATE_COLUMN = 'date'

# ConfiguraciÃ³n de visualizaciÃ³n
FIGSIZE = (14, 8)
DPI = 300
COLOR_PALETTE = 'viridis'
STYLE = 'seaborn-v0_8-darkgrid'
```

**Funciones principales:**
- `setup_directories()` - Crea directorios necesarios
- `get_config_summary()` - Muestra resumen de configuraciÃ³n
- `validate_paths()` - Valida existencia de archivos

**Â¿CuÃ¡ndo se usa?**
- Al inicio del pipeline para configurar rutas
- Cuando otros mÃ³dulos necesitan importar constantes
- Para cambiar parÃ¡metros globalmente sin editar mÃºltiples archivos

---

### ðŸ“¥ Extract/Extract.py - ExtracciÃ³n de Datos

**Â¿QuÃ© hace?**
Proporciona mÃºltiples formas de leer el archivo CSV de 77MB sin consumir toda la RAM.

**Clase Principal:** `DataExtractor`

**7 MÃ©todos de ExtracciÃ³n:**

1. **`extract_full()`** - Carga completa en memoria
   ```python
   extractor = DataExtractor("IntegratedData.csv")
   df = extractor.extract_full()
   ```

2. **`extract_chunks(chunk_size=100000)`** - Procesar por bloques
   ```python
   for chunk in extractor.extract_chunks(chunk_size=50000):
       procesar(chunk)  # Procesa 50,000 filas a la vez
   ```

3. **`extract_columns(columns)`** - Solo columnas especÃ­ficas
   ```python
   df = extractor.extract_columns(['date', 'cases', 'deaths'])
   ```

4. **`extract_sample(frac=0.1)`** - Muestreo aleatorio
   ```python
   df_test = extractor.extract_sample(frac=0.1)  # 10% de datos
   ```

5. **`extract_by_state(states)`** - Filtrar por estados
   ```python
   df_ca = extractor.extract_by_state(['California', 'Texas'])
   ```

6. **`extract_date_range(start, end)`** - Rango de fechas
   ```python
   df = extractor.extract_date_range('2021-03-01', '2021-03-31')
   ```

7. **`get_info()`** - InformaciÃ³n sin cargar datos
   ```python
   info = extractor.get_info()
   print(f"TamaÃ±o: {info['size_mb']} MB")
   ```

**Â¿Por quÃ© usar chunks?**
- Archivos grandes no caben en memoria RAM
- Permite procesar datasets de 10GB+ con solo 2GB de RAM
- MÃ¡s eficiente para operaciones secuenciales

---

### ðŸ§¹ Extract/Clean/Clean.py - Limpieza de Datos

**Â¿QuÃ© hace?**
Limpia y normaliza datos crudos automÃ¡ticamente usando procesamiento por chunks.

**FunciÃ³n Principal:**
```python
from Extract.Clean.Clean import clean_csv

clean_csv(
    input_csv="IntegratedData.csv",
    output_csv="Output/IntegratedData_cleaned.csv"
)
```

**Proceso de Limpieza:**

1. **NormalizaciÃ³n de columnas**
   - `Cases` â†’ `cases` (minÃºsculas)
   - `Daily Cases` â†’ `daily_cases` (sin espacios)
   - `2021-date` â†’ `date` (sin prefijos)

2. **Limpieza de valores**
   - Elimina espacios: `" Texas "` â†’ `"Texas"`
   - Convierte vacÃ­os a NaN: `""` â†’ `NaN`
   - Parsea fechas: `"2021-01-01"` â†’ `datetime`

3. **EliminaciÃ³n de duplicados**
   - Detecta filas idÃ©nticas
   - Mantiene primera ocurrencia
   - Usa streaming para no cargar todo en memoria

4. **EliminaciÃ³n de filas vacÃ­as**
   - Detecta filas donde TODAS las columnas son NaN
   - Las elimina para reducir tamaÃ±o del archivo

**Â¿CÃ³mo funciona el procesamiento por chunks?**
```python
# Lee 100,000 filas a la vez
for chunk in pd.read_csv(input_csv, chunksize=100000):
    chunk_limpio = clean_chunk(chunk)
    chunk_limpio.to_csv(output_csv, mode='append')
```

**Ventajas:**
- Procesa archivos de cualquier tamaÃ±o
- Memoria constante (no crece con el archivo)
- MÃ¡s rÃ¡pido que cargar todo en memoria

---

### ðŸ”„ Transform/Transform.py - TransformaciÃ³n de Datos

**Â¿QuÃ© hace?**
Calcula mÃ©tricas derivadas, agrega datos y realiza anÃ¡lisis estadÃ­stico.

**Clase Principal:** `DataTransformer`

**Funciones de TransformaciÃ³n:**

#### 1. Promedios MÃ³viles
```python
transformer = DataTransformer()
df = transformer.add_moving_average(df, column='daily_cases', window=7)
# AÃ±ade columna: daily_cases_ma7 (promedio de 7 dÃ­as)
```

**Â¿Para quÃ©?** Suavizar fluctuaciones diarias y ver tendencias reales.

#### 2. Tasas Derivadas
```python
# Tasa de mortalidad
df = transformer.calculate_mortality_rate(df)
# AÃ±ade: mortality_rate = (muertes / casos) * 100

# Tasa de crecimiento
df = transformer.calculate_growth_rate(df, column='cases')
# AÃ±ade: cases_growth_rate = cambio porcentual diario
```

**Â¿Para quÃ©?** Comparar severidad entre regiones sin depender del tamaÃ±o poblacional.

#### 3. Agregaciones
```python
# AgregaciÃ³n por fecha (suma nacional diaria)
df_nacional = transformer.aggregate_by_date(df)

# AgregaciÃ³n por estado
df_estados = transformer.aggregate_by_state(df)

# AgregaciÃ³n por condado
df_condados = transformer.aggregate_by_county(df)
```

**Â¿Para quÃ©?** AnÃ¡lisis a diferentes niveles geogrÃ¡ficos.

#### 4. Rankings
```python
# Top 10 estados con mÃ¡s casos
top_10 = transformer.get_top_states(df, metric='cases', n=10)

# Top 10 condados con mÃ¡s muertes
top_10_condados = transformer.get_top_counties(df, metric='deaths', n=10)
```

**Â¿Para quÃ©?** Identificar zonas mÃ¡s afectadas.

#### 5. Correlaciones
```python
# Matriz de correlaciÃ³n
corr = transformer.calculate_correlation(
    df, 
    columns=['cases', 'deaths', 'mobility_retail', 'mobility_transit']
)
```

**Â¿Para quÃ©?** Entender relaciones entre variables (movilidad â†’ casos).

#### 6. Features Temporales
```python
df = transformer.add_temporal_features(df)
# AÃ±ade: year, month, week, day_of_week, quarter, is_weekend
```

**Â¿Para quÃ©?** Detectar patrones estacionales y sesgos de reporte.

#### 7. NormalizaciÃ³n
```python
# Min-Max (escala 0-1)
df = transformer.normalize_minmax(df, columns=['cases'])

# Z-score (media=0, desv=1)
df = transformer.normalize_zscore(df, columns=['cases'])
```

**Â¿Para quÃ©?** Machine learning y comparaciÃ³n entre variables con diferentes escalas.

#### 8. DetecciÃ³n de Outliers
```python
# MÃ©todo IQR (rango intercuartil)
df = transformer.remove_outliers_iqr(df, column='cases')

# MÃ©todo Z-score (desviaciones estÃ¡ndar)
df = transformer.remove_outliers_zscore(df, column='cases', threshold=3)
```

**Â¿Para quÃ©?** Eliminar datos anÃ³malos que distorsionan anÃ¡lisis.

---

### ðŸ’¾ Load/Load.py - Persistencia de Datos

**Â¿QuÃ© hace?**
Guarda y carga datos procesados en mÃºltiples formatos con backups automÃ¡ticos.

**Clase Principal:** `DataLoader`

**Formatos Soportados:**
- CSV (`.csv`) - Compatible, liviano
- Excel (`.xlsx`) - Para usuarios no tÃ©cnicos
- JSON (`.json`) - APIs y web
- Parquet (`.parquet`) - MÃ¡s eficiente (compresiÃ³n y velocidad)

**Guardar Datos:**
```python
loader = DataLoader(output_dir="Output")

# CSV
loader.save_csv(df, "datos_procesados.csv")

# Excel con formato
loader.save_excel(df, "reporte.xlsx")

# JSON
loader.save_json(df, "api_data.json")

# Parquet (mÃ¡s rÃ¡pido, menor tamaÃ±o)
loader.save_parquet(df, "datos.parquet")
```

**Cargar Datos:**
```python
df = loader.load_csv("datos_procesados.csv")
df = loader.load_excel("reporte.xlsx")
df = loader.load_json("api_data.json")
df = loader.load_parquet("datos.parquet")
```

**Funciones Avanzadas:**

1. **Guardado por chunks (archivos grandes)**
   ```python
   loader.save_csv_chunks(df, "datos_grandes.csv", chunk_size=100000)
   ```

2. **Backups automÃ¡ticos**
   ```python
   loader.save_with_backup(df, "datos_importantes.csv")
   # Crea: datos_importantes_backup_20260205_143022.csv
   ```

3. **Guardar metadatos**
   ```python
   loader.save_metadata(df, "datos.csv")
   # Crea: datos_metadata.json con info del dataset
   ```

4. **Listar archivos**
   ```python
   files = loader.list_files()  # Lista todos los archivos en Output/
   info = loader.get_file_info("datos.csv")  # Info de un archivo
   ```

---

### ðŸ“Š Vizualize/plot.py - GeneraciÃ³n de Visualizaciones

**Â¿QuÃ© hace?**
Genera automÃ¡ticamente 11 grÃ¡ficas profesionales en espaÃ±ol de alta resoluciÃ³n.

**Generar Todas las GrÃ¡ficas:**
```python
from Vizualize.plot import generar_todas_las_graficas

generar_todas_las_graficas(
    csv_path="Output/IntegratedData_transformed.csv",
    output_dir="Output/figures"
)
```

**O desde lÃ­nea de comandos:**
```bash
python -m Vizualize.plot --input "Output/IntegratedData_cleaned.csv" --outdir "Output/figures"
```

**11 Funciones de VisualizaciÃ³n:**

1. `crear_serie_temporal_casos()` - EvoluciÃ³n temporal nacional
2. `crear_top_condados()` - Top 10 condados
3. `crear_scatter_casos_muertes()` - RelaciÃ³n casos vs muertes
4. `crear_correlacion_movilidad()` - CorrelaciÃ³n movilidad-casos
5. `crear_comparacion_dias()` - DÃ­as laborales vs fines de semana
6. `crear_top_estados()` - Top 10 estados
7. `crear_tasa_mortalidad_estados()` - Tasa de mortalidad por estado
8. `crear_evolucion_movilidad()` - Series temporales de movilidad
9. `crear_distribucion_dia_semana()` - DistribuciÃ³n por dÃ­a
10. `crear_promedio_movil()` - Promedio mÃ³vil de 7 dÃ­as
11. `crear_mapa_calor_correlacion()` - Matriz de correlaciÃ³n

**CaracterÃ­sticas de las grÃ¡ficas:**
- Alta resoluciÃ³n (300 DPI) - Listas para publicaciÃ³n
- Estilo profesional con seaborn
- Todas las etiquetas en espaÃ±ol
- Colores optimizados y accesibles
- Guardado automÃ¡tico en PNG

**Ejemplo de uso individual:**
```python
from Vizualize.plot import crear_serie_temporal_casos

crear_serie_temporal_casos(
    csv_path="Output/IntegratedData_transformed.csv",
    output_path="Output/figures/1_evolucion_casos_muertes.png"
)
```

---

### ðŸš€ pipeline.py - Orquestador del Pipeline ETL

**Â¿QuÃ© hace?**
Ejecuta todas las etapas del pipeline en el orden correcto automÃ¡ticamente.

**EjecuciÃ³n:**
```bash
# Ejecutar pipeline completo
python pipeline.py

# Ver configuraciÃ³n
python pipeline.py --show-config

# Especificar archivo de entrada
python pipeline.py --input MiArchivo.csv

# Sin archivos intermedios
python pipeline.py --skip-intermediate
```

**Flujo Completo:**
```python
# 1. ConfiguraciÃ³n
from Config.Config import *
setup_directories()

# 2. ExtracciÃ³n
from Extract.Extract import DataExtractor
extractor = DataExtractor("IntegratedData.csv")
df = extractor.extract_full()

# 3. Limpieza
from Extract.Clean.Clean import clean_csv
clean_csv("IntegratedData.csv", "Output/IntegratedData_cleaned.csv")

# 4. TransformaciÃ³n
from Transform.Transform import DataTransformer
transformer = DataTransformer()
df = pd.read_csv("Output/IntegratedData_cleaned.csv")
df = transformer.add_temporal_features(df)
df = transformer.add_moving_average(df, 'daily_cases', window=7)
df = transformer.calculate_mortality_rate(df)

# 5. Carga
from Load.Load import DataLoader
loader = DataLoader("Output")
loader.save_csv(df, "IntegratedData_transformed.csv")

# 6. Agregaciones
nacional = transformer.aggregate_by_date(df)
estados = transformer.get_top_states(df, n=10)
condados = transformer.get_top_counties(df, n=10)
loader.save_csv(nacional, "agregado_nacional.csv")
loader.save_csv(estados, "top_estados.csv")
loader.save_csv(condados, "top_condados.csv")

# 7. VisualizaciÃ³n
from Vizualize.plot import generar_todas_las_graficas
generar_todas_las_graficas(
    "Output/IntegratedData_transformed.csv",
    "Output/figures"
)
```

**Argumentos de lÃ­nea de comandos:**
- `--input`: Archivo CSV de entrada (default: `IntegratedData.csv`)
- `--output`: Directorio de salida (default: `Output/`)
- `--skip-intermediate`: No guardar archivos intermedios
- `--show-config`: Mostrar configuraciÃ³n y salir
- `--visualize`: Generar solo visualizaciones

---

## ðŸ“ Estructura del Proyecto

```
WilsonTrabajo1/
â”œâ”€â”€ Config/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ Config.py              # âš™ï¸ ConfiguraciÃ³n centralizada
â”‚
â”œâ”€â”€ Extract/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ Extract.py            # ðŸ“¥ 7 mÃ©todos de extracciÃ³n
â”‚   â””â”€â”€ Clean/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ Clean.py          # ðŸ§¹ Limpieza por chunks
â”‚
â”œâ”€â”€ Transform/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ Transform.py          # ðŸ”„ 15+ transformaciones
â”‚
â”œâ”€â”€ Load/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ Load.py               # ðŸ’¾ 4 formatos + backups
â”‚
â”œâ”€â”€ Vizualize/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ plot.py               # ðŸ“Š 11 grÃ¡ficas profesionales
â”‚
â”œâ”€â”€ Output/
â”‚   â”œâ”€â”€ IntegratedData_cleaned.csv
â”‚   â”œâ”€â”€ IntegratedData_transformed.csv
â”‚   â”œâ”€â”€ agregado_nacional.csv
â”‚   â”œâ”€â”€ top_estados.csv
â”‚   â”œâ”€â”€ top_condados.csv
â”‚   â””â”€â”€ figures/              # 11 visualizaciones PNG
â”‚
â”œâ”€â”€ pipeline.py               # ðŸš€ Orquestador principal
â”œâ”€â”€ IntegratedData.csv        # ðŸ“Š Dataset original (77MB)
â”œâ”€â”€ requirements.txt          # ðŸ“¦ Dependencias
â”œâ”€â”€ README.md                 # ðŸ“– Esta documentaciÃ³n (cÃ³digo)
â””â”€â”€ DATASET_INFO.md           # ðŸ“Š Info del dataset + grÃ¡ficas
```

---

## ðŸ› ï¸ TecnologÃ­as Utilizadas

- **Python 3.x** - Lenguaje de programaciÃ³n
- **Pandas** - ManipulaciÃ³n de datos y procesamiento por chunks
- **NumPy** - Operaciones numÃ©ricas y Ã¡lgebra lineal
- **Matplotlib** - Visualizaciones base
- **Seaborn** - GrÃ¡ficas estadÃ­sticas avanzadas

---

## ðŸš¦ GuÃ­a para Desarrolladores

### 1. Clonar y configurar

```bash
git clone https://github.com/kenmaroyert1/WilsonTrabajo1.git
cd WilsonTrabajo1
pip install -r requirements.txt
```

### 2. Familiarizarse con la configuraciÃ³n

```python
from Config.Config import *

print(f"Root: {PROJECT_ROOT}")
print(f"Chunk size: {CHUNK_SIZE}")
```

### 3. Probar mÃ³dulos individuales

```python
# ExtracciÃ³n
from Extract.Extract import DataExtractor
extractor = DataExtractor("IntegratedData.csv")
df_sample = extractor.extract_sample(frac=0.01)

# TransformaciÃ³n
from Transform.Transform import DataTransformer
transformer = DataTransformer()
df = transformer.add_moving_average(df, 'cases', window=7)
```

### 4. Ejecutar pipeline

```bash
python pipeline.py
```

### 5. Verificar resultados

- `Output/IntegratedData_cleaned.csv` - Datos limpios
- `Output/IntegratedData_transformed.csv` - Datos transformados
- `Output/figures/` - 11 grÃ¡ficas PNG

---

## ðŸ§ª Testing y Debugging

### Probar con muestra pequeÃ±a

```python
# Usar solo 1% de datos
extractor = DataExtractor("IntegratedData.csv")
df_test = extractor.extract_sample(frac=0.01)
df_test.to_csv("test_sample.csv", index=False)

# Ejecutar pipeline con muestra
python pipeline.py --input test_sample.csv
```

### Verificar procesamiento por chunks

```python
extractor = DataExtractor("IntegratedData.csv")
for i, chunk in enumerate(extractor.extract_chunks(chunk_size=10000)):
    print(f"Chunk {i}: {len(chunk)} filas")
    if i >= 5:
        break
```

### Validar transformaciones

```python
transformer = DataTransformer()
df = pd.read_csv("Output/IntegratedData_cleaned.csv", nrows=1000)

print("Antes:", df.shape)
df = transformer.add_temporal_features(df)
print("DespuÃ©s:", df.shape)
print("Nuevas columnas:", df.columns.tolist())
```

---

## ðŸ“– DocumentaciÃ³n Relacionada

- ðŸ“Š **[DATASET_INFO.md](DATASET_INFO.md)** - Para quÃ© sirve el dataset, visualizaciones con explicaciones, casos de uso reales

---

## ðŸ“ž Contacto

- **Repositorio:** https://github.com/kenmaroyert1/WilsonTrabajo1
- **Proyecto acadÃ©mico** - Universidad
