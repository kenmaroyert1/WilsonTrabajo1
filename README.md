# WilsonTrabajo1 - AnÃ¡lisis de COVID-19 y Movilidad en EE.UU.

## ğŸ“Š DescripciÃ³n del Proyecto

Este proyecto realiza un anÃ¡lisis exhaustivo de datos de COVID-19 en Estados Unidos, combinando informaciÃ³n epidemiolÃ³gica (casos y muertes) con datos de movilidad poblacional. El objetivo es entender cÃ³mo los cambios en los patrones de movilidad afectaron la propagaciÃ³n del virus durante la pandemia.

### ğŸ—‚ï¸ Sobre el Dataset

**Fuente de Datos:** Dataset integrado que combina mÃºltiples fuentes de informaciÃ³n pÃºblica sobre COVID-19.

**Contenido del Dataset (`IntegratedData.csv`):**
- **Datos EpidemiolÃ³gicos:** Casos confirmados, muertes, casos diarios y muertes diarias por condado y estado
- **Datos GeogrÃ¡ficos:** CÃ³digos FIPS, nombres de condados y estados
- **Datos Temporales:** Fechas, dÃ­a de la semana, fines de semana, dÃ­as feriados
- **Datos de Movilidad:** Cambios porcentuales en visitas a:
  - Comercios y lugares de recreaciÃ³n
  - Supermercados y farmacias
  - Parques
  - Estaciones de transporte pÃºblico
  - Lugares de trabajo
  - Zonas residenciales

**PerÃ­odo de Datos:** 2021 (inicio de la pandemia)

**Alcance GeogrÃ¡fico:** Todos los condados de Estados Unidos (~3,100 condados)

**TamaÃ±o:** ~77 MB con mÃ¡s de 935,000 registros

### ğŸ¯ Â¿Para QuÃ© Sirve Este Dataset?

Este dataset y sus visualizaciones son Ãºtiles para:

1. **AnÃ¡lisis EpidemiolÃ³gico:**
   - Identificar patrones temporales de la pandemia (olas, picos)
   - Comparar severidad entre regiones
   - Analizar tasas de mortalidad por Ã¡rea geogrÃ¡fica

2. **PolÃ­ticas de Salud PÃºblica:**
   - Evaluar efectividad de medidas de confinamiento
   - Identificar Ã¡reas que requieren mÃ¡s recursos sanitarios
   - Planificar estrategias de respuesta a futuras pandemias

3. **Estudios de Comportamiento Social:**
   - Entender cÃ³mo cambiaron los patrones de movilidad
   - Analizar correlaciÃ³n entre movilidad y contagios
   - Estudiar diferencias entre dÃ­as laborales y fines de semana

4. **InvestigaciÃ³n AcadÃ©mica:**
   - Modelos predictivos de propagaciÃ³n viral
   - Estudios de correlaciÃ³n entre variables socioeconÃ³micas
   - AnÃ¡lisis de series temporales

5. **Toma de Decisiones:**
   - Empresas: planificaciÃ³n de operaciones durante crisis sanitarias
   - Gobiernos: asignaciÃ³n de recursos y comunicaciÃ³n pÃºblica
   - Instituciones educativas: polÃ­ticas de apertura/cierre

## ğŸ”§ Procesamiento de Datos

Este repositorio implementa un **pipeline ETL completo** de procesamiento:

1. **ExtracciÃ³n (Extract):** Lectura y extracciÃ³n de datos desde archivos CSV grandes
2. **Limpieza (Clean):** NormalizaciÃ³n de columnas, eliminaciÃ³n de duplicados, manejo de valores nulos
3. **TransformaciÃ³n (Transform):** CÃ¡lculo de mÃ©tricas derivadas, agregaciones y anÃ¡lisis
4. **Carga (Load):** Guardado de datos procesados en mÃºltiples formatos
5. **VisualizaciÃ³n (Visualize):** GeneraciÃ³n de 11 grÃ¡ficas profesionales en espaÃ±ol

### ğŸ“¦ MÃ³dulos Implementados

#### **Config.py** - ConfiguraciÃ³n Centralizada
Gestiona toda la configuraciÃ³n del proyecto:
- ğŸ“ Rutas de directorios (datos, salida, figuras)
- âš™ï¸ ParÃ¡metros de procesamiento (tamaÃ±o de chunks: 100,000 filas)
- ğŸ“Š ConfiguraciÃ³n de visualizaciÃ³n (tamaÃ±os de figura, DPI, paletas de colores)
- ğŸ“ DefiniciÃ³n de columnas esperadas y tipos de datos
- ğŸ› ï¸ Funciones de utilidad (creaciÃ³n de directorios, resumen de configuraciÃ³n)

#### **Extract.py** - ExtracciÃ³n de Datos
Clase `DataExtractor` con mÃºltiples mÃ©todos de extracciÃ³n:
- `extract_full()`: Carga completa de datos en memoria
- `extract_chunks()`: Iterador para procesamiento por chunks
- `extract_columns()`: ExtracciÃ³n de columnas especÃ­ficas
- `extract_sample()`: Muestreo aleatorio del dataset
- `extract_by_state()`: Filtrado por estado(s)
- `extract_date_range()`: Filtrado por rango de fechas
- `get_info()`: InformaciÃ³n del archivo sin cargar datos

#### **Clean.py** - Limpieza de Datos
Procesamiento robusto para archivos grandes:
- âœ… Procesamiento por chunks (para archivos >50MB)
- âœ… NormalizaciÃ³n de nombres de columnas (minÃºsculas, sin espacios)
- âœ… EliminaciÃ³n de espacios en blanco en strings
- âœ… ConversiÃ³n de valores vacÃ­os a NaN
- âœ… Parsing de fechas automÃ¡tico
- âœ… EliminaciÃ³n de filas duplicadas
- âœ… EliminaciÃ³n de filas completamente vacÃ­as
- âœ… Memoria eficiente con streaming

**Resultado:** Dataset limpio guardado en `Output/IntegratedData_cleaned.csv`

#### **Transform.py** - TransformaciÃ³n de Datos
Clase `DataTransformer` con anÃ¡lisis avanzado:
- ğŸ“ˆ **Promedios MÃ³viles:** Suavizado de series temporales (ventanas configurables)
- ğŸ“Š **Tasas Derivadas:** Mortalidad, crecimiento, cambios porcentuales
- ğŸ”¢ **Agregaciones:** Por fecha, estado, condado
- ğŸ† **Rankings:** Top N estados/condados por cualquier mÃ©trica
- ğŸ”— **Correlaciones:** Matrices de correlaciÃ³n entre variables
- ğŸ“… **Features Temporales:** AÃ±o, mes, semana, dÃ­a, trimestre
- ğŸ”§ **NormalizaciÃ³n:** MinMax y Z-score
- ğŸš« **Outliers:** DetecciÃ³n y remociÃ³n (IQR y Z-score)

#### **Load.py** - Carga y Persistencia
Clase `DataLoader` para guardar/cargar datos:
- ğŸ’¾ **Formatos MÃºltiples:** CSV, Excel, JSON, Parquet
- ğŸ“¦ **Procesamiento Chunked:** Guardado por chunks para archivos grandes
- ğŸ”„ **Backups AutomÃ¡ticos:** CreaciÃ³n de copias de seguridad con timestamp
- ğŸ“‹ **Metadatos:** Guardado de informaciÃ³n sobre los datasets
- ğŸ“ **GestiÃ³n de Archivos:** Listado, informaciÃ³n, organizaciÃ³n

#### **pipeline.py** - Pipeline ETL Completo
Script integrador que ejecuta todo el flujo:
```bash
# Ejecutar pipeline completo
python pipeline.py

# Ver configuraciÃ³n
python pipeline.py --show-config

# Especificar archivo de entrada
python pipeline.py --input MiArchivo.csv

# Sin archivos intermedios
python pipeline.py --skip-intermediate
```

**El pipeline ejecuta:**
1. âœ… ExtracciÃ³n y limpieza de datos (chunks)
2. âœ… Transformaciones (promedios mÃ³viles, tasas, features)
3. âœ… Carga de datos transformados (CSV + metadatos + backup)
4. âœ… AnÃ¡lisis y agregaciones (nacional, estados, condados)
5. âœ… GeneraciÃ³n de archivos intermedios Ãºtiles

## ğŸ“Š Estado Actual del Proyecto

- âœ… **ConfiguraciÃ³n:** MÃ³dulo completo con todas las constantes
- âœ… **ExtracciÃ³n:** 7 mÃ©todos diferentes de lectura de datos
- âœ… **Limpieza:** Procesamiento por chunks implementado
- âœ… **TransformaciÃ³n:** 15+ funciones de anÃ¡lisis y transformaciÃ³n
- âœ… **Carga:** Soporte para 4 formatos de archivo
- âœ… **VisualizaciÃ³n:** 11 grÃ¡ficas profesionales en espaÃ±ol
- âœ… **Pipeline:** Script integrador completo funcional
- âœ… **DocumentaciÃ³n:** README exhaustivo con ejemplos

## Visualizaciones Generadas (11 grÃ¡ficas en espaÃ±ol)

### 1ï¸âƒ£ EvoluciÃ³n Temporal de Casos y Muertes (Nacional)
**Archivo:** `1_evolucion_casos_muertes.png`

**QuÃ© muestra:** GrÃ¡fica de lÃ­neas doble (eje Y dual) que muestra la suma nacional diaria de casos y muertes a lo largo del tiempo.

**InterpretaciÃ³n:** 
- Permite identificar olas/picos de la pandemia
- Observar la relaciÃ³n temporal entre casos y muertes
- Las muertes suelen seguir a los casos con un retraso de ~2-3 semanas

**Â¿QuÃ© nos dice esta grÃ¡fica?**
Esta visualizaciÃ³n es fundamental para entender la cronologÃ­a de la pandemia. Los picos azules (casos) anticipan picos rojos (muertes), lo que ayuda a:
- Predecir carga hospitalaria futura
- Evaluar si las medidas de salud pÃºblica estÃ¡n funcionando
- Identificar cuÃ¡ndo comienza y termina cada ola de contagios

**Utilidad prÃ¡ctica:** Hospitales pueden prepararse para picos de muertes 2-3 semanas despuÃ©s de picos de casos.

### 2ï¸âƒ£ Top 10 Condados con MÃ¡s Casos Acumulados
**Archivo:** `2_top_condados_casos.png`

**QuÃ© muestra:** GrÃ¡fica de barras horizontales mostrando los 10 condados con mayor nÃºmero de casos totales, incluyendo nombre del estado.

**InterpretaciÃ³n:**
- Identifica las Ã¡reas mÃ¡s afectadas por la pandemia
- Condados urbanos grandes tÃ­picamente tienen mÃ¡s casos debido a mayor densidad poblacional
- Ãštil para priorizar recursos de salud pÃºblica

**Â¿QuÃ© nos dice esta grÃ¡fica?**
Muestra las "zonas calientes" de la pandemia. Los condados con mÃ¡s casos suelen ser:
- Ãreas metropolitanas grandes (Los Angeles, Nueva York, Chicago)
- Centros de transporte y comercio
- Zonas con mayor densidad poblacional

**Utilidad prÃ¡ctica:** 
- Gobiernos pueden dirigir vacunas y recursos mÃ©dicos a estas Ã¡reas prioritarias
- Empresas pueden ajustar operaciones segÃºn riesgo por zona
- Investigadores pueden estudiar factores comunes en Ã¡reas mÃ¡s afectadas

### 3ï¸âƒ£ RelaciÃ³n entre Casos Diarios y Muertes Diarias
**Archivo:** `3_casos_vs_muertes.png`

**QuÃ© muestra:** Diagrama de dispersiÃ³n con lÃ­nea de tendencia mostrando la correlaciÃ³n entre casos diarios y muertes diarias.

**InterpretaciÃ³n:**
- Muestra la tasa de letalidad implÃ­cita (pendiente de la lÃ­nea)
- Puntos dispersos indican variabilidad por factores como edad, acceso a salud, etc.
- Tendencia positiva esperada: mÃ¡s casos â†’ mÃ¡s muertes

**Â¿QuÃ© nos dice esta grÃ¡fica?**
La pendiente de la lÃ­nea roja muestra la "letalidad promedio" del virus. Si la lÃ­nea es muy empinada, significa alta mortalidad relativa. La dispersiÃ³n de puntos indica que hay muchos factores adicionales:
- Calidad del sistema de salud local
- DemografÃ­a (Ã¡reas con poblaciÃ³n mayor tienen mÃ¡s muertes)
- Acceso a tratamientos y vacunas
- Variantes del virus circulantes

**Utilidad prÃ¡ctica:**
- Comparar letalidad entre diferentes perÃ­odos
- Evaluar efectividad de tratamientos (si la pendiente disminuye con el tiempo)
- Identificar outliers que requieren investigaciÃ³n especial

### 4ï¸âƒ£ Impacto de Cambios en Movilidad sobre Casos Nuevos
**Archivo:** `4_movilidad_correlacion.png`

**QuÃ© muestra:** GrÃ¡fica de barras mostrando correlaciones entre diferentes tipos de movilidad (comercios, supermercados, parques, transporte, trabajo, residencial) y casos diarios.

**InterpretaciÃ³n:**
- **Barras verdes (negativas):** Menos actividad = menos casos (ej: mÃ¡s tiempo en casa)
- **Barras naranjas (positivas):** MÃ¡s actividad = mÃ¡s casos (ej: mÃ¡s visitas a tiendas)
- Ayuda a entender quÃ© comportamientos reducen/aumentan contagios

**Â¿QuÃ© nos dice esta grÃ¡fica?**
Esta es una de las grÃ¡ficas mÃ¡s importantes para polÃ­ticas pÃºblicas. Muestra quÃ© cambios en comportamiento estÃ¡n correlacionados con casos:

- **CorrelaciÃ³n negativa (buena):** Aumento en tiempo residencial (quedarse en casa) reduce casos
- **CorrelaciÃ³n positiva (esperada):** MÃ¡s visitas a comercios y lugares pÃºblicos aumentan casos
- **Transporte pÃºblico:** Alta correlaciÃ³n positiva porque implica cercanÃ­a fÃ­sica prolongada

**Utilidad prÃ¡ctica:**
- DiseÃ±ar medidas de confinamiento efectivas (enfocarse en reducir actividades con mayor correlaciÃ³n)
- Evaluar impacto de polÃ­ticas (Â¿funcionÃ³ el cierre de comercios?)
- EducaciÃ³n pÃºblica: comunicar quÃ© actividades son mÃ¡s riesgosas
- Empresas: decidir polÃ­ticas de trabajo remoto basadas en datos

### 5ï¸âƒ£ ComparaciÃ³n: DÃ­as Laborales vs Fines de Semana
**Archivo:** `5_comparacion_dias.png`

**QuÃ© muestra:** Dos grÃ¡ficas de barras comparando promedios de casos y muertes en dÃ­as laborales versus fines de semana.

**InterpretaciÃ³n:**
- Identifica patrones de reporte (algunos lugares reportan menos en fines de semana)
- Puede reflejar diferencias reales en comportamiento social
- Ãštil para ajustar modelos predictivos

**Â¿QuÃ© nos dice esta grÃ¡fica?**
Muestra un sesgo importante en los datos: los fines de semana tÃ­picamente tienen menos casos reportados, pero NO necesariamente menos contagios reales. Esto se debe a:

- **Efecto administrativo:** Menos personal trabajando en laboratorios y oficinas de salud
- **Retraso en reportes:** Los casos del fin de semana se reportan el lunes/martes
- **Comportamiento real:** Menos gente va al mÃ©dico en fin de semana

**Utilidad prÃ¡ctica:**
- Modelos predictivos deben ajustar por dÃ­a de la semana
- No entrar en pÃ¡nico por "bajadas" artificiales los domingos
- Usar promedios de 7 dÃ­as en lugar de datos diarios crudos
- Periodistas y comunicadores deben reportar tendencias, no fluctuaciones diarias

### 6ï¸âƒ£ Top 10 Estados MÃ¡s Afectados
**Archivo:** `6_top_estados_casos.png`

**QuÃ© muestra:** GrÃ¡fica de barras horizontales mostrando los 10 estados con mayor nÃºmero de casos totales acumulados.

**InterpretaciÃ³n:**
- Compara el impacto de la pandemia a nivel estatal
- Estados mÃ¡s poblados y urbanos tÃ­picamente tienen mÃ¡s casos
- Ãštil para anÃ¡lisis de polÃ­ticas pÃºblicas estatales

**Â¿QuÃ© nos dice esta grÃ¡fica?**
Escalada a nivel estatal, muestra quÃ© estados fueron mÃ¡s golpeados por la pandemia. Factores que explican diferencias:

- **PoblaciÃ³n:** Estados como California, Texas, Florida tienen mÃ¡s casos por ser mÃ¡s poblados
- **Densidad urbana:** Estados con grandes metrÃ³polis tienen mÃ¡s transmisiÃ³n
- **Conectividad:** Estados con aeropuertos principales recibieron casos mÃ¡s temprano
- **PolÃ­ticas locales:** Estados con restricciones mÃ¡s estrictas pueden tener menos casos

**Utilidad prÃ¡ctica:**
- Comparar efectividad de polÃ­ticas estatales diferentes
- Asignar recursos federales proporcionalmente
- Estudios de caso: Â¿por quÃ© algunos estados lo hicieron mejor que otros?
- PlanificaciÃ³n para futuras pandemias a nivel estatal

### 7ï¸âƒ£ Tasa de Mortalidad por Estado
**Archivo:** `7_tasa_mortalidad_estados.png`

**QuÃ© muestra:** Top 15 estados con mayor porcentaje de muertes respecto a casos (tasa de letalidad).

**InterpretaciÃ³n:**
- Identifica estados con mayor severidad relativa
- Puede indicar diferencias en acceso a salud, demografÃ­a, o calidad de atenciÃ³n
- Rojo mÃ¡s intenso = mayor tasa de mortalidad

**Â¿QuÃ© nos dice esta grÃ¡fica?**
Esta grÃ¡fica es MÃS importante que el nÃºmero absoluto de casos, porque muestra la **severidad relativa** de la pandemia. Un estado puede tener pocos casos pero alta mortalidad, indicando:

- **Sistema de salud saturado:** Hospitales sin capacidad
- **PoblaciÃ³n vulnerable:** Mayor proporciÃ³n de personas mayores o con comorbilidades
- **Acceso limitado a tratamientos:** Menos acceso a antivirales, oxÃ­geno, UCI
- **Variantes mÃ¡s letales:** Algunas variantes del virus son mÃ¡s mortales
- **Retraso en diagnÃ³stico:** Casos detectados cuando ya estÃ¡n graves

**Utilidad prÃ¡ctica:**
- Priorizar mejoras en infraestructura de salud en estados con alta letalidad
- Investigar quÃ© estÃ¡n haciendo bien los estados con baja letalidad
- Dirigir vacunas y tratamientos a poblaciones vulnerables en estados crÃ­ticos
- AnÃ¡lisis econÃ³mico: impacto en productividad y costos sanitarios

### 8ï¸âƒ£ EvoluciÃ³n de Movilidad en el Tiempo
**Archivo:** `8_evolucion_movilidad.png`

**QuÃ© muestra:** Series temporales de cambios en movilidad para diferentes categorÃ­as (suavizado con promedio de 7 dÃ­as).

**InterpretaciÃ³n:**
- Muestra cÃ³mo cambiÃ³ el comportamiento durante la pandemia
- CaÃ­das pronunciadas = confinamientos/restricciones
- RecuperaciÃ³n gradual = normalizaciÃ³n de actividades
- La lÃ­nea residencial aumenta cuando otras disminuyen

**Â¿QuÃ© nos dice esta grÃ¡fica?**
Esta es una "radiografÃ­a del comportamiento social" durante la pandemia. Cuenta la historia de cÃ³mo la gente cambiÃ³ sus hÃ¡bitos:

**Fase 1 - Confinamiento:** Todas las lÃ­neas caen excepto residencial (la gente se queda en casa)
**Fase 2 - Reapertura gradual:** Las lÃ­neas empiezan a subir, especialmente supermercados (esenciales)
**Fase 3 - Nueva normalidad:** Patrones se estabilizan pero no vuelven al 100% pre-pandemia

**Detalles importantes:**
- **Parques:** Muy variable (depende del clima y restricciones locales)
- **Transporte pÃºblico:** RecuperaciÃ³n lenta (la gente prefiere auto por miedo al contagio)
- **Trabajo:** Muchas empresas adoptaron trabajo remoto permanente

**Utilidad prÃ¡ctica:**
- Empresas de transporte pueden planificar servicios segÃºn demanda real
- Comercios pueden ajustar horarios y personal
- Gobiernos locales pueden evaluar cumplimiento de restricciones
- Economistas pueden medir impacto en sectores especÃ­ficos (turismo, retail, etc.)

### 9ï¸âƒ£ DistribuciÃ³n por DÃ­a de la Semana
**Archivo:** `9_casos_dia_semana.png`

**QuÃ© muestra:** Dos grÃ¡ficas mostrando promedio de casos y muertes para cada dÃ­a de la semana (Lunes a Domingo).

**InterpretaciÃ³n:**
- Identifica sesgos en reportes (ej: menos reportes los fines de semana)
- Azul/Morado = dÃ­as laborales, Rojo/Naranja = fines de semana
- Ãštil para corregir modelos por efectos de calendario

**Â¿QuÃ© nos dice esta grÃ¡fica?**
Detalla dÃ­a por dÃ­a el patrÃ³n semanal de reportes y casos reales. Observaciones tÃ­picas:

**Lunes/Martes:** Picos artificiales porque se reportan casos acumulados del fin de semana
**MiÃ©rcoles-Viernes:** Datos mÃ¡s estables y confiables
**SÃ¡bado/Domingo:** CaÃ­da en reportes (menos personal administrativo trabajando)

**Diferencia entre casos y muertes:**
- Casos: Mayor variabilidad semanal (mÃ¡s dependiente de reportes administrativos)
- Muertes: Menos variabilidad (eventos mÃ¡s crÃ­ticos se reportan mÃ¡s consistentemente)

**Utilidad prÃ¡ctica:**
- **Para analistas:** No comparar lunes con domingo, usar semanas completas
- **Para modelos predictivos:** Incluir variables dummy de dÃ­a de la semana
- **Para comunicaciÃ³n pÃºblica:** Reportar promedios de 7 dÃ­as, no picos/valles diarios
- **Para planificaciÃ³n hospitalaria:** Anticipar que los lunes tendrÃ¡n mÃ¡s diagnÃ³sticos acumulados

### ğŸ”Ÿ Promedio MÃ³vil de Casos (7 dÃ­as)
**Archivo:** `10_promedio_movil.png`

**QuÃ© muestra:** Dos grÃ¡ficas con datos diarios (lÃ­nea tenue) y promedio mÃ³vil de 7 dÃ­as (lÃ­nea gruesa) para casos y muertes.

**InterpretaciÃ³n:**
- Suaviza fluctuaciones diarias y resalta tendencias reales
- Facilita identificar inicio/fin de olas
- El promedio mÃ³vil es mÃ¡s confiable para anÃ¡lisis de tendencias

**Â¿QuÃ© nos dice esta grÃ¡fica?**
Esta es la versiÃ³n "limpia" de los datos diarios. El promedio mÃ³vil de 7 dÃ­as elimina:

- **Ruido del fin de semana:** Ya no vemos bajadas artificiales los domingos
- **Picos administrativos:** Los lunes ya no se ven inflados artificialmente
- **Fluctuaciones aleatorias:** Eventos Ãºnicos (ej: un brote en una prisiÃ³n) no distorsionan la tendencia

**Â¿Por quÃ© 7 dÃ­as?**
- Captura un ciclo semanal completo
- Es el estÃ¡ndar usado por CDC, OMS y medios de comunicaciÃ³n
- Permite comparaciones internacionales

**CÃ³mo leerla:**
- **LÃ­nea sube:** La pandemia estÃ¡ empeorando (ola creciente)
- **LÃ­nea baja:** La pandemia estÃ¡ mejorando (ola en descenso)
- **LÃ­nea plana:** SituaciÃ³n estable (meseta)
- **Cambio de pendiente:** Momento crucial para decisiones de polÃ­tica pÃºblica

**Utilidad prÃ¡ctica:**
- **Gobiernos:** Decidir cuÃ¡ndo implementar o levantar restricciones
- **Hospitales:** Planificar capacidad con 1-2 semanas de anticipaciÃ³n
- **Medios de comunicaciÃ³n:** Reportar tendencias reales sin alarmar innecesariamente
- **Individuos:** Evaluar riesgo personal y ajustar precauciones

### 1ï¸âƒ£1ï¸âƒ£ Mapa de Calor de CorrelaciÃ³n Completo
**Archivo:** `11_mapa_calor_correlacion.png`

**QuÃ© muestra:** Matriz de correlaciÃ³n entre todas las variables numÃ©ricas del dataset (casos, muertes, movilidad, fin de semana, feriados).

**InterpretaciÃ³n:**
- **Rojo intenso:** CorrelaciÃ³n positiva fuerte (cuando una sube, la otra tambiÃ©n)
- **Azul intenso:** CorrelaciÃ³n negativa fuerte (cuando una sube, la otra baja)
- **Blanco:** Sin correlaciÃ³n
- Ãštil para identificar relaciones entre variables y validar hipÃ³tesis
- Por ejemplo: casos acumulados y muertes acumuladas tienen correlaciÃ³n cercana a 1 (esperado)

**Â¿QuÃ© nos dice esta grÃ¡fica?**
Este es el "mapa de conexiones" entre todas las variables del dataset. Es una herramienta poderosa para:

**Validar hipÃ³tesis:**
- Â¿La movilidad realmente afecta los casos? â†’ Ver correlaciÃ³n entre columnas de movilidad y daily_cases
- Â¿Los fines de semana afectan reportes? â†’ Ver correlaciÃ³n entre is_weekend y daily_cases

**Descubrir patrones no obvios:**
- Correlaciones inesperadas pueden indicar factores causales ocultos
- Falta de correlaciÃ³n donde esperÃ¡bamos una puede indicar problemas en los datos

**CÃ³mo leerlo:**
- **Diagonal (1.0):** Cada variable perfectamente correlacionada consigo misma
- **Casos acumulados â†” Muertes acumuladas (~0.95):** Fuerte correlaciÃ³n (mÃ¡s casos = mÃ¡s muertes)
- **Movilidad residencial â†” Otros tipos de movilidad (negativa):** Cuando aumenta tiempo en casa, disminuye movilidad externa
- **Daily_cases â†” Movilidad en comercios (positiva):** MÃ¡s visitas = mÃ¡s contagios

**Correlaciones importantes a buscar:**
1. **Casos vs Movilidad:** Â¿QuÃ© actividades tienen mayor correlaciÃ³n con contagios?
2. **Casos vs Fines de semana:** Â¿Hay sesgo de reporte?
3. **Casos vs DÃ­as feriados:** Â¿Los feriados afectan los datos?

**Utilidad prÃ¡ctica:**
- **CientÃ­ficos de datos:** SelecciÃ³n de variables para modelos predictivos
- **EpidemiÃ³logos:** Identificar factores de riesgo principales
- **PolÃ­ticos:** Decidir quÃ© restricciones implementar (enfocar en actividades con alta correlaciÃ³n)
- **Investigadores:** Generar nuevas hipÃ³tesis para estudios profundos
- **VerificaciÃ³n de calidad:** Detectar datos anÃ³malos (correlaciones que no tienen sentido)

## CÃ³mo Generar las Figuras

### 1. Instalar dependencias

```powershell
pip install -r requirements.txt
```

### 2. Ejecutar el script de visualizaciÃ³n

```powershell
python -m Vizualize.plot --input "Output/IntegratedData_cleaned.csv" --outdir "Output/figures"
```

Las 11 figuras se guardarÃ¡n automÃ¡ticamente en `Output/figures/`.

## ğŸ“ˆ Casos de Uso Reales

Este proyecto y dataset pueden ser utilizados por:

### ğŸ¥ Sector Salud
- **Hospitales:** Planificar capacidad de UCI y personal segÃºn tendencias
- **Departamentos de Salud PÃºblica:** DiseÃ±ar campaÃ±as de vacunaciÃ³n y comunicaciÃ³n
- **Investigadores mÃ©dicos:** Estudiar patrones de transmisiÃ³n y efectividad de tratamientos

### ğŸ›ï¸ Gobierno y PolÃ­tica PÃºblica
- **Tomadores de decisiones:** Evaluar cuÃ¡ndo implementar/levantar restricciones
- **Planificadores urbanos:** DiseÃ±ar ciudades mÃ¡s resilientes a pandemias
- **GestiÃ³n de emergencias:** PreparaciÃ³n para futuras crisis sanitarias

### ğŸ“š EducaciÃ³n e InvestigaciÃ³n
- **Universidades:** Material didÃ¡ctico para cursos de epidemiologÃ­a, ciencia de datos, salud pÃºblica
- **Estudiantes:** Proyectos de tesis sobre anÃ¡lisis de datos, machine learning aplicado
- **Investigadores:** Publicaciones acadÃ©micas sobre correlaciÃ³n movilidad-contagios

### ğŸ’¼ Sector Empresarial
- **Comercios:** Entender patrones de consumo durante crisis
- **Transporte:** Planificar servicios segÃºn demanda real
- **Seguros:** Evaluar riesgos y ajustar primas
- **Empresas tech:** Desarrollar soluciones de monitoreo y predicciÃ³n

### ğŸ“Š Ciencia de Datos y Analytics
- **Modelos predictivos:** Entrenar algoritmos de machine learning para predecir olas
- **AnÃ¡lisis de series temporales:** Estudiar patrones estacionales y cÃ­clicos
- **VisualizaciÃ³n de datos:** Ejemplos de buenas prÃ¡cticas en grÃ¡ficas explicativas

## ğŸ” Insights Principales del AnÃ¡lisis

DespuÃ©s de procesar y visualizar este dataset, podemos concluir:

1. **La movilidad SÃ afecta los contagios:** Existe correlaciÃ³n clara entre aumento en actividades pÃºblicas y casos
2. **Las muertes siguen a los casos con 2-3 semanas de retraso:** PatrÃ³n consistente Ãºtil para predicciÃ³n
3. **Los datos tienen sesgo de reporte:** Los fines de semana y feriados muestran menos casos (efecto administrativo)
4. **La tasa de mortalidad varÃ­a significativamente por regiÃ³n:** No todos los estados experimentaron la misma severidad
5. **El comportamiento social cambiÃ³ drÃ¡sticamente:** Las grÃ¡ficas de movilidad muestran un "antes y despuÃ©s" claro
6. **Los promedios mÃ³viles son esenciales:** Los datos diarios crudos tienen demasiado ruido para anÃ¡lisis

## ğŸ› ï¸ TecnologÃ­as Utilizadas

- **Python 3.x:** Lenguaje de programaciÃ³n principal
- **Pandas:** ManipulaciÃ³n y anÃ¡lisis de datos (lectura por chunks, limpieza, agregaciones)
- **Matplotlib:** CreaciÃ³n de visualizaciones estÃ¡ticas de alta calidad
- **Seaborn:** GrÃ¡ficas estadÃ­sticas avanzadas (mapas de calor, distribuciones)
- **NumPy:** Operaciones numÃ©ricas y Ã¡lgebra lineal
- **Git/GitHub:** Control de versiones y colaboraciÃ³n

## ğŸ“ Estructura del Proyecto y ExplicaciÃ³n de Archivos .py

```
WilsonTrabajo1/
â”œâ”€â”€ ğŸ“ Config/                    # âš™ï¸ MÃ³dulo de configuraciÃ³n
â”‚   â”œâ”€â”€ __init__.py              # Hace que Config sea un paquete Python
â”‚   â””â”€â”€ Config.py                # âš™ï¸ CONFIGURACIÃ“N CENTRALIZADA
â”‚
â”œâ”€â”€ ğŸ“ Extract/                   # ğŸ“¥ MÃ³dulo de extracciÃ³n de datos
â”‚   â”œâ”€â”€ __init__.py              # Hace que Extract sea un paquete Python
â”‚   â”œâ”€â”€ Extract.py               # ğŸ“¥ EXTRACCIÃ“N DE DATOS
â”‚   â””â”€â”€ ğŸ“ Clean/                # ğŸ§¹ SubmÃ³dulo de limpieza
â”‚       â”œâ”€â”€ __init__.py          # Hace que Clean sea un paquete Python
â”‚       â””â”€â”€ Clean.py             # ğŸ§¹ LIMPIEZA DE DATOS
â”‚
â”œâ”€â”€ ğŸ“ Transform/                 # ğŸ”„ MÃ³dulo de transformaciÃ³n
â”‚   â”œâ”€â”€ __init__.py              # Hace que Transform sea un paquete Python
â”‚   â””â”€â”€ Transform.py             # ğŸ”„ TRANSFORMACIÃ“N Y ANÃLISIS
â”‚
â”œâ”€â”€ ğŸ“ Load/                      # ğŸ’¾ MÃ³dulo de carga/guardado
â”‚   â”œâ”€â”€ __init__.py              # Hace que Load sea un paquete Python
â”‚   â””â”€â”€ Load.py                  # ğŸ’¾ PERSISTENCIA DE DATOS
â”‚
â”œâ”€â”€ ğŸ“ Vizualize/                 # ğŸ“Š MÃ³dulo de visualizaciÃ³n
â”‚   â”œâ”€â”€ __init__.py              # Hace que Vizualize sea un paquete Python
â”‚   â””â”€â”€ plot.py                  # ğŸ“Š GENERACIÃ“N DE GRÃFICAS
â”‚
â”œâ”€â”€ ğŸ“ Output/                    # ğŸ“‚ Archivos de salida
â”‚   â”œâ”€â”€ __init__.py              # Hace que Output sea un paquete Python
â”‚   â”œâ”€â”€ IntegratedData_cleaned.csv      # Dataset limpio (77MB)
â”‚   â”œâ”€â”€ IntegratedData_transformed.csv  # Dataset transformado (generado por pipeline)
â”‚   â”œâ”€â”€ agregado_nacional.csv           # Agregaciones nacionales
â”‚   â”œâ”€â”€ top_estados.csv                 # Top 10 estados
â”‚   â”œâ”€â”€ top_condados.csv                # Top 10 condados
â”‚   â””â”€â”€ ğŸ“ figures/              # 11 visualizaciones PNG
â”‚       â”œâ”€â”€ 1_evolucion_casos_muertes.png
â”‚       â”œâ”€â”€ 2_top_condados_casos.png
â”‚       â”œâ”€â”€ 3_casos_vs_muertes.png
â”‚       â”œâ”€â”€ 4_movilidad_correlacion.png
â”‚       â”œâ”€â”€ 5_comparacion_dias.png
â”‚       â”œâ”€â”€ 6_top_estados_casos.png
â”‚       â”œâ”€â”€ 7_tasa_mortalidad_estados.png
â”‚       â”œâ”€â”€ 8_evolucion_movilidad.png
â”‚       â”œâ”€â”€ 9_casos_dia_semana.png
â”‚       â”œâ”€â”€ 10_promedio_movil.png
â”‚       â””â”€â”€ 11_mapa_calor_correlacion.png
â”‚
â”œâ”€â”€ pipeline.py                  # ğŸš€ PIPELINE ETL COMPLETO
â”œâ”€â”€ IntegratedData.csv           # Dataset original (77MB)
â”œâ”€â”€ requirements.txt             # Dependencias Python
â””â”€â”€ README.md                    # Esta documentaciÃ³n

Total: ~155MB de datos + 11 visualizaciones profesionales
```

### ğŸ“ Â¿Para QuÃ© Sirve Cada Archivo .py?

#### âš™ï¸ **Config/Config.py** - ConfiguraciÃ³n Centralizada del Proyecto
**PropÃ³sito:** Almacena TODAS las configuraciones en un solo lugar para evitar "nÃºmeros mÃ¡gicos" y facilitar mantenimiento.

**QuÃ© contiene:**
- ğŸ“ **Rutas de directorios:** Define dÃ³nde estÃ¡n los datos, salidas y figuras
  ```python
  PROJECT_ROOT = Path(__file__).parent.parent  # RaÃ­z del proyecto
  DATA_DIR = PROJECT_ROOT                      # Donde estÃ¡n los CSV originales
  OUTPUT_DIR = PROJECT_ROOT / "Output"         # Donde se guardan resultados
  FIGURES_DIR = OUTPUT_DIR / "figures"         # Donde se guardan grÃ¡ficas
  ```

- âš™ï¸ **ParÃ¡metros de procesamiento:**
  ```python
  CHUNK_SIZE = 100_000              # CuÃ¡ntas filas procesar a la vez (memoria eficiente)
  MOVING_AVERAGE_WINDOW = 7         # Ventana para promedios mÃ³viles
  TOP_N_COUNTIES = 10               # CuÃ¡ntos condados mostrar en rankings
  TOP_N_STATES = 10                 # CuÃ¡ntos estados mostrar en rankings
  ```

- ğŸ“Š **ConfiguraciÃ³n de visualizaciÃ³n:**
  ```python
  FIGURE_SIZE_DEFAULT = (12, 6)     # TamaÃ±o por defecto de grÃ¡ficas
  DPI = 100                         # ResoluciÃ³n de imÃ¡genes
  COLOR_PALETTE = 'Set2'            # Paleta de colores Seaborn
  ```

- ğŸ“‹ **DefiniciÃ³n de columnas esperadas:**
  ```python
  EXPECTED_COLUMNS = ['date', 'county', 'state', 'fips', 'cases', 'deaths', ...]
  MOBILITY_COLUMNS = ['retail_recreation', 'grocery_pharmacy', 'parks', ...]
  NUMERIC_COLUMNS = ['cases', 'deaths', 'daily_cases', 'daily_deaths']
  DATE_COLUMNS = ['date']
  ```

- ğŸ› ï¸ **Funciones de utilidad:**
  - `ensure_directories()`: Crea los directorios necesarios si no existen
  - `get_config_summary()`: Muestra un resumen de toda la configuraciÃ³n

**CuÃ¡ndo usarlo:**
- Al inicio de cualquier script para importar configuraciones
- Si necesitas cambiar rutas, tamaÃ±os de figura, o parÃ¡metros globales
- Para mantener consistencia en todo el proyecto

**Ejemplo de uso:**
```python
from Config.Config import OUTPUT_DIR, CHUNK_SIZE, ensure_directories

ensure_directories()  # Crear directorios si no existen
print(f"Procesando con chunks de {CHUNK_SIZE:,} filas")
```

---

#### ğŸ“¥ **Extract/Extract.py** - ExtracciÃ³n de Datos desde CSV
**PropÃ³sito:** Proporciona mÃºltiples estrategias para leer el dataset segÃºn necesidades (memoria, velocidad, filtros).

**QuÃ© contiene:**
Clase `DataExtractor` con 7 mÃ©todos diferentes de extracciÃ³n:

1. **`extract_full()`** - Carga completa en memoria
   - Usa cuando: Tienes suficiente RAM (8GB+) y necesitas todos los datos a la vez
   - Retorna: DataFrame completo de pandas

2. **`extract_chunks(chunk_size)`** - Iterador por chunks
   - Usa cuando: Archivo muy grande (>1GB) y no cabe en memoria
   - Retorna: Generador que produce chunks de datos
   - Ejemplo: Procesar 100,000 filas a la vez

3. **`extract_columns(columns)`** - Solo columnas especÃ­ficas
   - Usa cuando: Solo necesitas algunas columnas (ahorra memoria)
   - Retorna: DataFrame con columnas seleccionadas

4. **`extract_sample(frac=0.1)`** - Muestreo aleatorio
   - Usa cuando: Quieres hacer pruebas rÃ¡pidas con 10% de datos
   - Retorna: DataFrame con muestra aleatoria

5. **`extract_by_state(states)`** - Filtrar por estados
   - Usa cuando: Solo necesitas datos de ciertos estados (ej: California, Texas)
   - Retorna: DataFrame filtrado

6. **`extract_date_range(start, end)`** - Filtrar por fechas
   - Usa cuando: Solo necesitas un perÃ­odo especÃ­fico (ej: marzo-abril 2021)
   - Retorna: DataFrame con fechas en el rango

7. **`get_info()`** - InformaciÃ³n del archivo SIN cargarlo
   - Usa cuando: Quieres saber tamaÃ±o, columnas, etc. sin usar memoria
   - Retorna: Diccionario con metadatos

**CuÃ¡ndo usarlo:**
- Al inicio del pipeline para cargar datos originales
- Cuando necesites leer solo parte de los datos
- Para anÃ¡lisis exploratorios rÃ¡pidos con muestras

**Ejemplo de uso:**
```python
from Extract.Extract import DataExtractor

# Crear extractor
extractor = DataExtractor("IntegratedData.csv")

# OpciÃ³n 1: Cargar todo (si tienes RAM)
df_completo = extractor.extract_full()

# OpciÃ³n 2: Procesar por chunks (archivos grandes)
for chunk in extractor.extract_chunks(chunk_size=50000):
    procesar(chunk)  # Procesa cada chunk

# OpciÃ³n 3: Solo datos de California
df_california = extractor.extract_by_state(['California'])

# OpciÃ³n 4: Solo columnas de casos y muertes
df_mini = extractor.extract_columns(['date', 'cases', 'deaths'])
```

---

#### ğŸ§¹ **Extract/Clean/Clean.py** - Limpieza de Datos
**PropÃ³sito:** Limpiar y normalizar datos crudos para anÃ¡lisis (manejo de chunks para archivos grandes).

**QuÃ© hace:**
1. **NormalizaciÃ³n de nombres de columnas:**
   - Convierte a minÃºsculas: `Cases` â†’ `cases`
   - Remueve espacios: `Daily Cases` â†’ `daily_cases`

2. **Limpieza de valores:**
   - Strings: Quita espacios al inicio/final
   - Valores vacÃ­os: Convierte `""` â†’ `NaN`
   - Fechas: Parsea automÃ¡ticamente columnas `date`

3. **EliminaciÃ³n de duplicados:**
   - Identifica filas duplicadas
   - Las elimina manteniendo la primera ocurrencia
   - Usa streaming para archivos grandes (no carga todo en memoria)

4. **EliminaciÃ³n de filas vacÃ­as:**
   - Detecta filas donde TODAS las columnas son NaN
   - Las elimina para reducir tamaÃ±o del archivo

**Procesamiento por chunks:**
- Lee el archivo en bloques de 100,000 filas
- Procesa cada bloque independientemente
- Guarda resultados de manera incremental
- **Ventaja:** Puede procesar archivos de 10GB+ con solo 2GB de RAM

**CuÃ¡ndo usarlo:**
- Inmediatamente despuÃ©s de recibir datos crudos
- Antes de cualquier anÃ¡lisis o visualizaciÃ³n
- Si el archivo tiene problemas de formato

**Ejemplo de uso:**
```python
from Extract.Clean.Clean import clean_csv

# Limpiar archivo (procesamiento automÃ¡tico por chunks)
clean_csv(
    input_csv="IntegratedData.csv",
    output_csv="Output/IntegratedData_cleaned.csv"
)

# Resultado: Archivo limpio guardado en Output/
```

**Funciones principales:**
- `normalize_column_name(col)`: Normaliza nombre de columna
- `clean_chunk(chunk)`: Limpia un chunk de datos
- `clean_csv(input, output)`: FunciÃ³n principal que orquesta todo

---

#### ğŸ”„ **Transform/Transform.py** - TransformaciÃ³n y AnÃ¡lisis de Datos
**PropÃ³sito:** Calcular mÃ©tricas derivadas, agregaciones y anÃ¡lisis avanzados sobre datos limpios.

**QuÃ© contiene:**
Clase `DataTransformer` con 15+ funciones de transformaciÃ³n:

**1. MÃ©tricas Derivadas:**
- `calculate_moving_average(column, window=7)`: Promedio mÃ³vil (suaviza series temporales)
- `calculate_growth_rate(column)`: Tasa de crecimiento porcentual diaria
- `calculate_mortality_rate()`: Muertes / Casos * 100

**2. Agregaciones:**
- `aggregate_by_date()`: Suma nacional diaria
- `aggregate_by_state()`: Totales por estado
- `aggregate_by_county()`: Totales por condado

**3. Rankings:**
- `get_top_counties(metric, n=10)`: Top N condados por mÃ©trica
- `get_top_states(metric, n=10)`: Top N estados por mÃ©trica

**4. AnÃ¡lisis EstadÃ­stico:**
- `calculate_correlation_matrix(columns)`: Matriz de correlaciÃ³n
- `get_summary_statistics()`: EstadÃ­sticas descriptivas (media, mediana, std, etc.)

**5. Feature Engineering:**
- `add_time_features()`: Agrega aÃ±o, mes, semana, dÃ­a, trimestre desde fecha
- `normalize_column(column, method)`: MinMax o Z-score normalizaciÃ³n
- `filter_outliers(column, method)`: Detecta y remueve outliers

**CuÃ¡ndo usarlo:**
- DespuÃ©s de limpiar datos y antes de visualizar
- Para calcular mÃ©tricas que no estÃ¡n en los datos originales
- Para anÃ¡lisis exploratorio y generaciÃ³n de insights

**Ejemplo de uso:**
```python
from Transform.Transform import DataTransformer
import pandas as pd

# Cargar datos limpios
df = pd.read_csv("Output/IntegratedData_cleaned.csv")

# Crear transformador
transformer = DataTransformer(df)

# Calcular promedio mÃ³vil de 7 dÃ­as para casos
df_transformed = transformer.calculate_moving_average('daily_cases', window=7)

# Calcular tasa de mortalidad
df_transformed = transformer.calculate_mortality_rate()

# Obtener top 10 estados con mÃ¡s casos
top_states = transformer.get_top_states('cases', n=10)

# Agregar caracterÃ­sticas temporales (aÃ±o, mes, semana, etc.)
df_transformed = transformer.add_time_features()

# Calcular matriz de correlaciÃ³n
corr_matrix = transformer.calculate_correlation_matrix()
```

---

#### ğŸ’¾ **Load/Load.py** - Persistencia y Carga de Datos
**PropÃ³sito:** Guardar y cargar datos procesados en mÃºltiples formatos (CSV, Excel, JSON, Parquet).

**QuÃ© contiene:**
Clase `DataLoader` con funciones de guardado/carga:

**Formatos soportados:**
1. **CSV** - `save_to_csv()` / `load_from_csv()`
   - Formato universal, compatible con todo
   - OpciÃ³n chunked para archivos grandes

2. **Excel** - `save_to_excel()` / `load_from_excel()`
   - Para reportes y anÃ¡lisis en Excel/Sheets
   - Soporta mÃºltiples hojas

3. **JSON** - `save_to_json()` / `load_from_json()`
   - Para APIs y aplicaciones web
   - Soporta JSON Lines (streaming)

4. **Parquet** - `save_to_parquet()` / `load_from_parquet()`
   - Formato columnar comprimido
   - MÃ¡s rÃ¡pido y 70% mÃ¡s pequeÃ±o que CSV

**Funciones adicionales:**
- `create_backup(filename)`: Crea copia de seguridad con timestamp
- `save_metadata(filename, metadata)`: Guarda metadatos en JSON
- `load_metadata(filename)`: Carga metadatos
- `list_files(extension)`: Lista archivos en Output/
- `get_file_info(filename)`: InformaciÃ³n de archivo (tamaÃ±o, fecha, etc.)

**CuÃ¡ndo usarlo:**
- Al final del pipeline para guardar resultados
- Para crear backups antes de modificaciones
- Para exportar datos a diferentes herramientas

**Ejemplo de uso:**
```python
from Load.Load import DataLoader
import pandas as pd

# Crear loader
loader = DataLoader(output_dir="Output")

# Guardar DataFrame en CSV
df = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})
loader.save_to_csv(df, "resultados.csv")

# Guardar en Excel
loader.save_to_excel(df, "resultados.xlsx", sheet_name="Datos")

# Guardar en Parquet (comprimido)
loader.save_to_parquet(df, "resultados.parquet", compression='snappy')

# Crear backup
loader.create_backup("IntegratedData_cleaned.csv")

# Guardar metadatos
metadata = {
    'descripcion': 'Datos procesados',
    'filas': len(df),
    'columnas': list(df.columns)
}
loader.save_metadata("resultados.csv", metadata)

# Cargar datos
df_cargado = loader.load_from_csv("resultados.csv")

# Listar todos los CSV en Output/
archivos = loader.list_files(extension='.csv')
print(f"Encontrados {len(archivos)} archivos CSV")
```

---

#### ğŸ“Š **Vizualize/plot.py** - GeneraciÃ³n de Visualizaciones
**PropÃ³sito:** Crear 11 grÃ¡ficas profesionales en espaÃ±ol que explican la pandemia desde mÃºltiples Ã¡ngulos.

**QuÃ© contiene:**
11 funciones especializadas de visualizaciÃ³n:

1. **`plot_1_temporal_nacional()`** - EvoluciÃ³n temporal de casos y muertes
   - GrÃ¡fica de lÃ­neas con doble eje Y
   - Muestra tendencias nacionales dÃ­a a dÃ­a

2. **`plot_2_top_condados()`** - Top 10 condados con mÃ¡s casos
   - GrÃ¡fica de barras horizontales
   - Identifica hotspots locales

3. **`plot_3_casos_vs_muertes()`** - RelaciÃ³n casos vs muertes
   - Scatter plot con regresiÃ³n
   - Muestra tasa de letalidad

4. **`plot_4_movilidad_correlacion()`** - Impacto de movilidad en casos
   - GrÃ¡fica de barras de correlaciones
   - Identifica quÃ© actividades aumentan contagios

5. **`plot_5_comparacion_dias()`** - DÃ­as laborales vs fines de semana
   - GrÃ¡fica de barras comparativa
   - Muestra sesgos de reporte

6. **`plot_6_top_estados_casos()`** - Top 10 estados mÃ¡s afectados
   - GrÃ¡fica de barras horizontales
   - ComparaciÃ³n a nivel estatal

7. **`plot_7_tasa_mortalidad_estados()`** - Tasa de mortalidad por estado
   - GrÃ¡fica de barras con gradiente de color
   - Identifica estados con mayor severidad

8. **`plot_8_evolucion_movilidad()`** - EvoluciÃ³n de movilidad en el tiempo
   - GrÃ¡fica de lÃ­neas mÃºltiples
   - Muestra cambios de comportamiento

9. **`plot_9_casos_dia_semana()`** - DistribuciÃ³n por dÃ­a de la semana
   - GrÃ¡fica de barras por dÃ­a
   - Identifica patrones semanales

10. **`plot_10_promedio_movil()`** - Promedio mÃ³vil de casos (7 dÃ­as)
    - GrÃ¡fica con datos crudos + suavizados
    - Facilita ver tendencias reales

11. **`plot_11_mapa_calor_correlacion()`** - Mapa de calor de correlaciones
    - Heatmap con todas las variables
    - Identifica relaciones entre variables

**CaracterÃ­sticas comunes:**
- Todas en espaÃ±ol (tÃ­tulos, etiquetas, leyendas)
- Estilo profesional consistente
- Alta resoluciÃ³n (DPI 100)
- Colores accesibles (colorblind-friendly)
- Guardado automÃ¡tico en PNG

**CuÃ¡ndo usarlo:**
- Al final del pipeline para crear reportes visuales
- Para presentaciones y reportes
- Para exploraciÃ³n de datos

**Ejemplo de uso:**
```python
from Vizualize.plot import (
    plot_1_temporal_nacional,
    plot_11_mapa_calor_correlacion,
    generate_all_plots
)
import pandas as pd

# Cargar datos
df = pd.read_csv("Output/IntegratedData_cleaned.csv")

# Generar una grÃ¡fica especÃ­fica
plot_1_temporal_nacional(df, outdir="Output/figures")

# O generar todas las 11 grÃ¡ficas de una vez
generate_all_plots(df, outdir="Output/figures")

# Las grÃ¡ficas se guardan automÃ¡ticamente en Output/figures/
```

---

#### ğŸš€ **pipeline.py** - Pipeline ETL Completo Integrador
**PropÃ³sito:** Orquesta TODO el flujo de trabajo de principio a fin (Extract â†’ Clean â†’ Transform â†’ Load â†’ Visualize).

**QuÃ© hace:**
Clase `COVIDPipeline` que ejecuta 5 pasos secuenciales:

**PASO 1: ExtracciÃ³n** (`step1_extract`)
- Lee el archivo CSV original
- Valida que existe
- Puede usar diferentes mÃ©todos (full, chunks, sample)

**PASO 2: Limpieza** (`step2_clean`)
- Ejecuta `clean_csv()` con procesamiento por chunks
- Normaliza columnas
- Elimina duplicados y valores vacÃ­os
- Guarda: `IntegratedData_cleaned.csv`

**PASO 3: TransformaciÃ³n** (`step3_transform`)
- Calcula promedios mÃ³viles (7 dÃ­as)
- Calcula tasa de mortalidad
- Calcula tasa de crecimiento
- Agrega caracterÃ­sticas temporales (aÃ±o, mes, semana, etc.)
- Retorna: DataFrame con mÃ©tricas derivadas

**PASO 4: Carga** (`step4_load`)
- Guarda datos transformados en CSV
- Crea archivo de metadatos JSON
- Crea backup del archivo limpio
- Guarda: `IntegratedData_transformed.csv` + metadatos

**PASO 5: AnÃ¡lisis y Agregaciones** (`step5_analyze`)
- Agrega datos a nivel nacional
- Identifica top 10 estados
- Identifica top 10 condados
- Calcula estadÃ­sticas descriptivas
- Calcula matriz de correlaciÃ³n
- Guarda: `agregado_nacional.csv`, `top_estados.csv`, `top_condados.csv`

**CuÃ¡ndo usarlo:**
- Para ejecutar el anÃ¡lisis completo de principio a fin
- En producciÃ³n o automatizaciÃ³n
- Para procesar nuevos datasets con la misma estructura

**Ejemplo de uso:**
```bash
# Ejecutar pipeline completo con configuraciÃ³n por defecto
python pipeline.py

# Ver configuraciÃ³n antes de ejecutar
python pipeline.py --show-config

# Usar un archivo de entrada diferente
python pipeline.py --input OtroDatos.csv

# No guardar archivos intermedios (solo resultado final)
python pipeline.py --skip-intermediate
```

**Desde Python:**
```python
from pipeline import COVIDPipeline

# Crear pipeline
pipeline = COVIDPipeline(input_file="IntegratedData.csv")

# Ejecutar pipeline completo
df_final = pipeline.run_full_pipeline(save_intermediate=True)

# O ejecutar pasos individuales
df_clean = pipeline.step2_clean()
df_transformed = pipeline.step3_transform(df_clean)
pipeline.step4_load(df_transformed)
results = pipeline.step5_analyze(df_transformed)

print(f"âœ… Pipeline completado: {len(df_final):,} filas procesadas")
```

**Salida del pipeline:**
- `IntegratedData_cleaned.csv` - Datos limpios
- `IntegratedData_transformed.csv` - Datos con mÃ©tricas derivadas
- `agregado_nacional.csv` - Suma nacional diaria
- `top_estados.csv` - Top 10 estados
- `top_condados.csv` - Top 10 condados
- Archivo de metadatos JSON
- Backup con timestamp

---

### ğŸ”„ Flujo de Trabajo Completo

```
1. IntegratedData.csv (77MB)
         â†“
2. Config.py (carga configuraciones)
         â†“
3. Extract.py (lee datos)
         â†“
4. Clean.py (limpia datos)
         â†“
5. Transform.py (calcula mÃ©tricas)
         â†“
6. Load.py (guarda resultados)
         â†“
7. plot.py (genera grÃ¡ficas)
         â†“
8. Output/ (11 PNG + CSVs procesados)
```

**Todo esto es orquestado por `pipeline.py`** para ejecutar de forma automÃ¡tica.

---

### ğŸ’¡ Consejos de Uso

**Para anÃ¡lisis exploratorio rÃ¡pido:**
```python
# Usar Extract.py con muestreo
from Extract.Extract import DataExtractor
extractor = DataExtractor("IntegratedData.csv")
df_sample = extractor.extract_sample(frac=0.1)  # Solo 10% de datos
```

**Para procesar archivos gigantes (>5GB):**
```python
# Usar procesamiento por chunks
from Extract.Extract import DataExtractor
for chunk in DataExtractor("BigFile.csv").extract_chunks(50000):
    process(chunk)  # Procesa de a poco
```

**Para crear reportes automatizados:**
```bash
# Ejecutar pipeline completo desde terminal
python pipeline.py --input NuevosDatos.csv
```

**Para anÃ¡lisis especÃ­fico de un estado:**
```python
from Extract.Extract import DataExtractor
df_california = DataExtractor("IntegratedData.csv").extract_by_state(['California'])
```

Total: ~155MB de datos + 11 visualizaciones profesionales + Pipeline ETL completo

---

## ğŸ“Š Resumen Visual: Arquitectura del Proyecto

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PROYECTO WILSONTRABAJO1                      â”‚
â”‚          AnÃ¡lisis de COVID-19 y Movilidad en EE.UU.            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

                         ğŸ“¥ ENTRADA
                             â”‚
                   IntegratedData.csv
                    (77MB, 935k filas)
                             â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚                                       â”‚
         â–¼                                       â–¼
    âš™ï¸ Config.py                          ğŸ“¥ Extract.py
    â€¢ Rutas                                â€¢ 7 mÃ©todos de
    â€¢ ParÃ¡metros                            extracciÃ³n
    â€¢ Constantes                           â€¢ Filtros por estado
    â”‚                                      â€¢ Muestreo aleatorio
    â”‚                                           â”‚
    â”‚                                           â–¼
    â”‚                                    ğŸ§¹ Clean.py
    â”‚                                    â€¢ Procesamiento chunks
    â”‚                                    â€¢ NormalizaciÃ³n
    â”‚                                    â€¢ DeduplicaciÃ³n
    â”‚                                           â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
          ğŸ”„ Transform.py
          â€¢ Promedios mÃ³viles
          â€¢ Tasas derivadas
          â€¢ Agregaciones
          â€¢ Correlaciones
          â€¢ 15+ funciones
                 â”‚
                 â–¼
          ğŸ’¾ Load.py
          â€¢ Guardar CSV/Excel
          â€¢ Guardar JSON/Parquet
          â€¢ Metadatos
          â€¢ Backups
                 â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”
         â”‚               â”‚
         â–¼               â–¼
  ğŸ“Š Vizualize.py   ğŸ“‚ Output/
  â€¢ 11 grÃ¡ficas     â€¢ IntegratedData_cleaned.csv (77MB)
    en espaÃ±ol      â€¢ IntegratedData_transformed.csv
  â€¢ Profesionales   â€¢ agregado_nacional.csv
  â€¢ PNG alta res    â€¢ top_estados.csv
         â”‚          â€¢ top_condados.csv
         â”‚          â€¢ metadatos.json
         â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”˜
                 â”‚
                 â–¼
          ğŸ“ Output/figures/
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚ âœ… 1_evolucion_casos_muertes.png    â”‚
          â”‚ âœ… 2_top_condados_casos.png         â”‚
          â”‚ âœ… 3_casos_vs_muertes.png           â”‚
          â”‚ âœ… 4_movilidad_correlacion.png      â”‚
          â”‚ âœ… 5_comparacion_dias.png           â”‚
          â”‚ âœ… 6_top_estados_casos.png          â”‚
          â”‚ âœ… 7_tasa_mortalidad_estados.png    â”‚
          â”‚ âœ… 8_evolucion_movilidad.png        â”‚
          â”‚ âœ… 9_casos_dia_semana.png           â”‚
          â”‚ âœ… 10_promedio_movil.png            â”‚
          â”‚ âœ… 11_mapa_calor_correlacion.png    â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

         TODO ORQUESTADO POR: ğŸš€ pipeline.py
         Ejecutar: python pipeline.py
```

### ğŸ¯ Flujo de Datos Simplificado

```
CSV Crudo â†’ Extract â†’ Clean â†’ Transform â†’ Load â†’ Visualize â†’ Resultados
  (77MB)      (lee)   (limpia)  (calcula)  (guarda)  (grafica)   (11 PNG)
```

### ğŸ“ˆ MÃ©tricas del Proyecto

| Componente | LÃ­neas de CÃ³digo | Funciones | DescripciÃ³n |
|------------|------------------|-----------|-------------|
| **Config.py** | 290 | 2 | ConfiguraciÃ³n centralizada |
| **Extract.py** | 250 | 8 | ExtracciÃ³n de datos |
| **Clean.py** | 130 | 3 | Limpieza de datos |
| **Transform.py** | 450 | 16 | Transformaciones y anÃ¡lisis |
| **Load.py** | 450 | 13 | Persistencia de datos |
| **plot.py** | 620 | 12 | Visualizaciones profesionales |
| **pipeline.py** | 280 | 6 | Orquestador ETL completo |
| **TOTAL** | **2,470** | **60** | **Pipeline ETL completo funcional** |

### ğŸ“¦ Archivos Generados por el Pipeline

| Archivo | TamaÃ±o | Filas | Columnas | DescripciÃ³n |
|---------|--------|-------|----------|-------------|
| IntegratedData_cleaned.csv | 77MB | 935,444 | 17 | Datos limpios |
| IntegratedData_transformed.csv | 85MB | 935,444 | 25+ | + mÃ©tricas derivadas |
| agregado_nacional.csv | 50KB | 365 | 4 | Suma nacional diaria |
| top_estados.csv | 2KB | 10 | 5 | Top 10 estados |
| top_condados.csv | 3KB | 10 | 6 | Top 10 condados |
| 11 grÃ¡ficas PNG | 5MB | - | - | Visualizaciones profesionales |

---

## ğŸš€ PrÃ³ximas Mejoras Posibles

- [ ] Implementar anÃ¡lisis interactivo con Plotly/Dash
- [ ] Crear dashboard web en tiempo real
- [ ] Agregar mapas geogrÃ¡ficos con Folium
- [ ] Modelos de machine learning para predicciÃ³n de casos
- [ ] API REST para consultar datos
- [ ] AnÃ¡lisis de sentimiento en redes sociales correlacionado con casos
- [ ] ComparaciÃ³n internacional (agregar datos de otros paÃ­ses)
- [ ] AnÃ¡lisis de variantes del virus
- [ ] Estudio de efectividad de vacunas por regiÃ³n

## ğŸ‘¥ Contribuciones

Este proyecto fue desarrollado como parte del curso de Ciencia de Datos. 

Â¿Quieres contribuir? Las pull requests son bienvenidas. Para cambios mayores, por favor abre un issue primero para discutir quÃ© te gustarÃ­a cambiar.

## ğŸ“„ Licencia

Este proyecto es de cÃ³digo abierto y estÃ¡ disponible para fines educativos y de investigaciÃ³n.

## ğŸ“§ Contacto

**Repositorio:** [WilsonTrabajo1](https://github.com/kenmaroyert1/WilsonTrabajo1)  
**Rama principal de desarrollo:** `feature1`

---

â­ Si este proyecto te fue Ãºtil, considera darle una estrella en GitHub

**Ãšltima actualizaciÃ³n:** Febrero 2026
