# WilsonTrabajo1 - Pipeline ETL para AnÃ¡lisis de COVID-19

## ğŸš€ Inicio RÃ¡pido (Quick Start)

### Requisitos Previos
- Python 3.7 o superior
- Dataset: `IntegratedData.csv` (colocar en la raÃ­z del proyecto)

### InstalaciÃ³n en 3 Pasos

```bash
# 1. Clonar el repositorio
git clone https://github.com/kenmaroyert1/WilsonTrabajo1.git
cd WilsonTrabajo1

# 2. Instalar dependencias
pip install -r requirements.txt

# 3. Ejecutar el pipeline completo
python pipeline.py
```

### âœ… Resultado
DespuÃ©s de ejecutar `pipeline.py`, obtendrÃ¡s:
- âœ”ï¸ Datos limpios: `Output/IntegratedData_cleaned.csv`
- âœ”ï¸ Datos transformados: `Output/IntegratedData_transformed.csv`
- âœ”ï¸ 11 grÃ¡ficas profesionales en: `Output/figures/`
- âœ”ï¸ Agregaciones: `Output/agregado_nacional.csv`, `top_estados.csv`, `top_condados.csv`

### ğŸ“Š Ejecutar Solo Visualizaciones

Si ya tienes los datos procesados:
```python
from Vizualize.plot import *
from Config.Config import OUTPUT_DIR, FIGURES_DIR

# Generar todas las grÃ¡ficas
crear_serie_temporal_casos(OUTPUT_DIR / "IntegratedData_transformed.csv")
crear_mapa_calor_movilidad(OUTPUT_DIR / "IntegratedData_transformed.csv")
# ... mÃ¡s funciones disponibles
```

---

## ğŸ“– DescripciÃ³n del Proyecto

Este proyecto implementa un **pipeline ETL completo** para el anÃ¡lisis de datos de COVID-19 en Estados Unidos, combinando informaciÃ³n epidemiolÃ³gica (casos y muertes) con datos de movilidad poblacional.

### ğŸ¯ Objetivo
Procesar, analizar y visualizar grandes volÃºmenes de datos sobre la pandemia para entender la relaciÃ³n entre los cambios en patrones de movilidad y la propagaciÃ³n del virus.

### ğŸ“š DocumentaciÃ³n Adicional
- **[DATASET_INFO.md](DATASET_INFO.md)** - InformaciÃ³n detallada sobre el dataset, visualizaciones, casos de uso e interpretaciÃ³n de grÃ¡ficas

---

## ğŸ”§ Arquitectura del Pipeline ETL

El proyecto sigue una arquitectura modular con 5 etapas principales:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   EXTRACT   â”‚ --> â”‚    CLEAN    â”‚ --> â”‚  TRANSFORM  â”‚ --> â”‚    LOAD     â”‚ --> â”‚  VISUALIZE  â”‚
â”‚   Lectura   â”‚     â”‚  Limpieza   â”‚     â”‚  AnÃ¡lisis   â”‚     â”‚  Guardado   â”‚     â”‚  GrÃ¡ficas   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1. **Extract** - ExtracciÃ³n de Datos
- Lectura eficiente de archivos CSV grandes (77MB+)
- Procesamiento por chunks para optimizar memoria
- MÃºltiples mÃ©todos de extracciÃ³n (completo, por partes, filtrado)

### 2. **Clean** - Limpieza de Datos
- NormalizaciÃ³n de nombres de columnas
- EliminaciÃ³n de duplicados y valores nulos
- Streaming para archivos grandes (>50MB)

### 3. **Transform** - TransformaciÃ³n y AnÃ¡lisis
- CÃ¡lculo de mÃ©tricas derivadas (tasas, promedios mÃ³viles)
- Agregaciones temporales y geogrÃ¡ficas
- DetecciÃ³n y manejo de outliers

### 4. **Load** - Persistencia de Datos
- Guardado en mÃºltiples formatos (CSV, Excel, JSON, Parquet)
- Backups automÃ¡ticos con timestamp
- GestiÃ³n de metadatos

### 5. **Visualize** - GeneraciÃ³n de GrÃ¡ficas
- 11 visualizaciones profesionales en espaÃ±ol
- GrÃ¡ficas de alta resoluciÃ³n (300 DPI)
- Interpretaciones detalladas

---

## ğŸ“¦ MÃ³dulos Implementados

### âš™ï¸ **Config/Config.py** - ConfiguraciÃ³n Centralizada

**PropÃ³sito:** Gestionar toda la configuraciÃ³n del proyecto desde un solo lugar.

**Contiene:**
```python
# Rutas de directorios
PROJECT_ROOT = Path(__file__).parent.parent
DATA_DIR = PROJECT_ROOT / "Data"
OUTPUT_DIR = PROJECT_ROOT / "Output"
FIGURES_DIR = OUTPUT_DIR / "figures"

# ParÃ¡metros de procesamiento
CHUNK_SIZE = 100000  # Filas por chunk para archivos grandes
DATE_COLUMN = 'date'
FIGSIZE = (14, 8)
DPI = 300

# ConfiguraciÃ³n de visualizaciÃ³n
COLOR_PALETTE = 'viridis'
STYLE = 'seaborn-v0_8-darkgrid'
```

**Funciones principales:**
- `setup_directories()`: Crea directorios necesarios
- `get_config_summary()`: Muestra resumen de configuraciÃ³n
- `validate_paths()`: Valida existencia de archivos/carpetas

**CuÃ¡ndo usarlo:**
- Importar constantes en otros mÃ³dulos
- Cambiar rutas de archivos
- Ajustar parÃ¡metros de procesamiento

---

### ğŸ“¥ **Extract/Extract.py** - ExtracciÃ³n de Datos

**PropÃ³sito:** Proporcionar mÃºltiples formas de leer datos del CSV inicial.

**Clase:** `DataExtractor`

**MÃ©todos disponibles:**

1. **`extract_full()`** - Carga completa en memoria
   - Usa cuando: Tienes suficiente RAM (>8GB)
   - Retorna: DataFrame completo

2. **`extract_chunks(chunk_size=100000)`** - Iterador por chunks
   - Usa cuando: Archivo muy grande o poca RAM
   - Retorna: Iterador de DataFrames

3. **`extract_columns(columns)`** - Solo columnas especÃ­ficas
   - Usa cuando: Solo necesitas algunas columnas
   - Retorna: DataFrame con columnas seleccionadas

4. **`extract_sample(frac=0.1)`** - Muestreo aleatorio
   - Usa cuando: Pruebas rÃ¡pidas con 10% de datos
   - Retorna: DataFrame con muestra aleatoria

5. **`extract_by_state(states)`** - Filtrar por estados
   - Usa cuando: Solo necesitas datos de ciertos estados
   - Retorna: DataFrame filtrado

6. **`extract_date_range(start, end)`** - Filtrar por fechas
   - Usa cuando: Solo necesitas un perÃ­odo especÃ­fico
   - Retorna: DataFrame con fechas en el rango

7. **`get_info()`** - InformaciÃ³n sin cargar datos
   - Usa cuando: Quieres saber tamaÃ±o, columnas sin usar memoria
   - Retorna: Diccionario con metadatos

**Ejemplo de uso:**
```python
from Extract.Extract import DataExtractor

extractor = DataExtractor("IntegratedData.csv")

# OpciÃ³n 1: Cargar todo
df_completo = extractor.extract_full()

# OpciÃ³n 2: Procesar por chunks (archivos grandes)
for chunk in extractor.extract_chunks(chunk_size=50000):
    procesar(chunk)

# OpciÃ³n 3: Solo datos de California
df_california = extractor.extract_by_state(['California'])
```

---

### ğŸ§¹ **Extract/Clean/Clean.py** - Limpieza de Datos

**PropÃ³sito:** Limpiar y normalizar datos crudos para anÃ¡lisis.

**QuÃ© hace:**

1. **NormalizaciÃ³n de columnas:**
   - `Cases` â†’ `cases`
   - `Daily Cases` â†’ `daily_cases`

2. **Limpieza de valores:**
   - Quita espacios en blanco
   - Convierte valores vacÃ­os a NaN
   - Parsea fechas automÃ¡ticamente

3. **EliminaciÃ³n de duplicados:**
   - Identifica y elimina filas duplicadas
   - Mantiene primera ocurrencia

4. **Procesamiento por chunks:**
   - Lee en bloques de 100,000 filas
   - Procesa cada bloque independientemente
   - Puede procesar archivos de 10GB+ con 2GB de RAM

**FunciÃ³n principal:**
```python
from Extract.Clean.Clean import clean_csv

clean_csv(
    input_csv="IntegratedData.csv",
    output_csv="Output/IntegratedData_cleaned.csv"
)
```

**Funciones auxiliares:**
- `normalize_column_name(col)`: Normaliza nombre de columna
- `clean_chunk(chunk)`: Limpia un chunk de datos
- `remove_duplicates_chunked()`: Elimina duplicados en streaming

---

### ğŸ”„ **Transform/Transform.py** - TransformaciÃ³n de Datos

**PropÃ³sito:** Calcular mÃ©tricas derivadas y realizar anÃ¡lisis avanzado.

**Clase:** `DataTransformer`

**Transformaciones disponibles:**

1. **Promedios MÃ³viles**
   ```python
   df = transformer.add_moving_average(df, column='daily_cases', window=7)
   # AÃ±ade columna: daily_cases_ma7
   ```

2. **Tasas Derivadas**
   ```python
   df = transformer.calculate_mortality_rate(df)
   # AÃ±ade: mortality_rate (muertes/casos * 100)
   
   df = transformer.calculate_growth_rate(df, column='cases')
   # AÃ±ade: cases_growth_rate
   ```

3. **Agregaciones**
   ```python
   # AgregaciÃ³n por fecha
   df_daily = transformer.aggregate_by_date(df)
   
   # AgregaciÃ³n por estado
   df_state = transformer.aggregate_by_state(df)
   
   # AgregaciÃ³n por condado
   df_county = transformer.aggregate_by_county(df)
   ```

4. **Rankings**
   ```python
   # Top 10 estados con mÃ¡s casos
   top_states = transformer.get_top_states(df, metric='cases', n=10)
   
   # Top 10 condados con mÃ¡s muertes
   top_counties = transformer.get_top_counties(df, metric='deaths', n=10)
   ```

5. **Correlaciones**
   ```python
   # Matriz de correlaciÃ³n
   corr_matrix = transformer.calculate_correlation(df, columns=['cases', 'deaths', 'mobility'])
   ```

6. **Features Temporales**
   ```python
   df = transformer.add_temporal_features(df)
   # AÃ±ade: year, month, week, day_of_week, quarter, is_weekend
   ```

7. **NormalizaciÃ³n**
   ```python
   # Min-Max (0-1)
   df = transformer.normalize_minmax(df, columns=['cases', 'deaths'])
   
   # Z-score (media=0, std=1)
   df = transformer.normalize_zscore(df, columns=['cases', 'deaths'])
   ```

8. **DetecciÃ³n de Outliers**
   ```python
   # MÃ©todo IQR (InterQuartile Range)
   df = transformer.remove_outliers_iqr(df, column='cases')
   
   # MÃ©todo Z-score
   df = transformer.remove_outliers_zscore(df, column='cases', threshold=3)
   ```

**Ejemplo completo:**
```python
from Transform.Transform import DataTransformer
import pandas as pd

transformer = DataTransformer()
df = pd.read_csv("Output/IntegratedData_cleaned.csv")

# Aplicar mÃºltiples transformaciones
df = transformer.add_temporal_features(df)
df = transformer.add_moving_average(df, 'daily_cases', window=7)
df = transformer.calculate_mortality_rate(df)

# Guardar datos transformados
df.to_csv("Output/IntegratedData_transformed.csv", index=False)
```

---

### ğŸ’¾ **Load/Load.py** - Persistencia de Datos

**PropÃ³sito:** Guardar y cargar datos procesados en mÃºltiples formatos.

**Clase:** `DataLoader`

**Formatos soportados:**
- CSV (`.csv`)
- Excel (`.xlsx`)
- JSON (`.json`)
- Parquet (`.parquet`)

**Funciones principales:**

1. **Guardar datos**
   ```python
   from Load.Load import DataLoader
   
   loader = DataLoader(output_dir="Output")
   
   # Guardar en CSV
   loader.save_csv(df, "datos_procesados.csv")
   
   # Guardar en Excel con formato
   loader.save_excel(df, "datos_procesados.xlsx")
   
   # Guardar en JSON
   loader.save_json(df, "datos_procesados.json")
   
   # Guardar en Parquet (mÃ¡s eficiente)
   loader.save_parquet(df, "datos_procesados.parquet")
   ```

2. **Cargar datos**
   ```python
   # Cargar desde CSV
   df = loader.load_csv("datos_procesados.csv")
   
   # Cargar desde Excel
   df = loader.load_excel("datos_procesados.xlsx")
   
   # Cargar desde JSON
   df = loader.load_json("datos_procesados.json")
   
   # Cargar desde Parquet
   df = loader.load_parquet("datos_procesados.parquet")
   ```

3. **Guardado por chunks (archivos grandes)**
   ```python
   loader.save_csv_chunks(df, "datos_grandes.csv", chunk_size=100000)
   ```

4. **Backups automÃ¡ticos**
   ```python
   loader.save_with_backup(df, "datos_importantes.csv")
   # Crea: datos_importantes_backup_20260205_143022.csv
   ```

5. **Guardar metadatos**
   ```python
   loader.save_metadata(df, "datos_procesados.csv")
   # Crea: datos_procesados_metadata.json con info del dataset
   ```

6. **GestiÃ³n de archivos**
   ```python
   # Listar archivos en Output/
   files = loader.list_files()
   
   # Obtener informaciÃ³n de un archivo
   info = loader.get_file_info("datos_procesados.csv")
   ```

---

### ğŸ“Š **Vizualize/plot.py** - GeneraciÃ³n de Visualizaciones

**PropÃ³sito:** Generar 11 grÃ¡ficas profesionales en espaÃ±ol para anÃ¡lisis de COVID-19.

**Funciones de visualizaciÃ³n:**

1. **`crear_serie_temporal_casos(csv_path)`**
   - EvoluciÃ³n temporal de casos y muertes (eje dual)
   - Archivo: `1_evolucion_casos_muertes.png`

2. **`crear_top_condados(csv_path)`**
   - Top 10 condados con mÃ¡s casos
   - Archivo: `2_top_condados_casos.png`

3. **`crear_scatter_casos_muertes(csv_path)`**
   - RelaciÃ³n casos vs muertes (scatter + tendencia)
   - Archivo: `3_casos_vs_muertes.png`

4. **`crear_correlacion_movilidad(csv_path)`**
   - CorrelaciÃ³n movilidad vs casos
   - Archivo: `4_movilidad_correlacion.png`

5. **`crear_comparacion_dias(csv_path)`**
   - ComparaciÃ³n dÃ­as laborales vs fines de semana
   - Archivo: `5_comparacion_dias.png`

6. **`crear_top_estados(csv_path)`**
   - Top 10 estados mÃ¡s afectados
   - Archivo: `6_top_estados_casos.png`

7. **`crear_tasa_mortalidad_estados(csv_path)`**
   - Tasa de mortalidad por estado (top 15)
   - Archivo: `7_tasa_mortalidad_estados.png`

8. **`crear_evolucion_movilidad(csv_path)`**
   - EvoluciÃ³n temporal de movilidad (todas las categorÃ­as)
   - Archivo: `8_evolucion_movilidad.png`

9. **`crear_distribucion_dia_semana(csv_path)`**
   - DistribuciÃ³n de casos y muertes por dÃ­a de semana
   - Archivo: `9_casos_dia_semana.png`

10. **`crear_promedio_movil(csv_path)`**
    - Promedio mÃ³vil de 7 dÃ­as (casos y muertes)
    - Archivo: `10_promedio_movil.png`

11. **`crear_mapa_calor_correlacion(csv_path)`**
    - Matriz de correlaciÃ³n completa (heatmap)
    - Archivo: `11_mapa_calor_correlacion.png`

**Generar todas las grÃ¡ficas:**
```python
from Vizualize.plot import generar_todas_las_graficas

generar_todas_las_graficas(
    csv_path="Output/IntegratedData_transformed.csv",
    output_dir="Output/figures"
)
```

**O ejecutar desde lÃ­nea de comandos:**
```bash
python -m Vizualize.plot --input "Output/IntegratedData_cleaned.csv" --outdir "Output/figures"
```

**CaracterÃ­sticas:**
- Alta resoluciÃ³n (300 DPI)
- Estilo profesional con seaborn
- Todas las etiquetas en espaÃ±ol
- Colores optimizados para publicaciÃ³n
- TamaÃ±os de figura configurables

---

### ğŸš€ **pipeline.py** - Pipeline ETL Completo

**PropÃ³sito:** Orquestar todas las etapas del procesamiento en un solo script.

**QuÃ© hace:**
1. âœ… Carga configuraciÃ³n
2. âœ… Extrae datos del CSV original
3. âœ… Limpia datos (chunks)
4. âœ… Aplica transformaciones
5. âœ… Guarda datos procesados
6. âœ… Genera agregaciones
7. âœ… Crea visualizaciones

**Uso bÃ¡sico:**
```bash
# Ejecutar pipeline completo
python pipeline.py

# Ver configuraciÃ³n
python pipeline.py --show-config

# Especificar archivo de entrada
python pipeline.py --input MiArchivo.csv

# Sin archivos intermedios
python pipeline.py --skip-intermediate
```

**Argumentos disponibles:**
- `--input`: Archivo CSV de entrada (default: `IntegratedData.csv`)
- `--output`: Directorio de salida (default: `Output/`)
- `--skip-intermediate`: No guardar archivos intermedios
- `--show-config`: Mostrar configuraciÃ³n y salir
- `--visualize`: Generar solo visualizaciones (sin procesar)

**Flujo del pipeline:**
```python
# 1. ExtracciÃ³n
extractor = DataExtractor(input_file)
df = extractor.extract_full()

# 2. Limpieza
clean_csv(input_file, cleaned_file)
df = pd.read_csv(cleaned_file)

# 3. TransformaciÃ³n
transformer = DataTransformer()
df = transformer.add_temporal_features(df)
df = transformer.add_moving_average(df, 'daily_cases', window=7)
df = transformer.calculate_mortality_rate(df)

# 4. Carga
loader = DataLoader(output_dir)
loader.save_csv(df, "IntegratedData_transformed.csv")

# 5. Agregaciones
nacional = transformer.aggregate_by_date(df)
estados = transformer.get_top_states(df, n=10)
condados = transformer.get_top_counties(df, n=10)

# 6. VisualizaciÃ³n
generar_todas_las_graficas(transformed_file, figures_dir)
```

---

## ğŸ“ Estructura del Proyecto

```
WilsonTrabajo1/
â”œâ”€â”€ ğŸ“ Config/                    # âš™ï¸ ConfiguraciÃ³n
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ Config.py                # Constantes y configuraciÃ³n global
â”‚
â”œâ”€â”€ ğŸ“ Extract/                   # ğŸ“¥ ExtracciÃ³n de datos
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ Extract.py               # Clase DataExtractor (7 mÃ©todos)
â”‚   â””â”€â”€ ğŸ“ Clean/                # ğŸ§¹ Limpieza de datos
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ Clean.py             # Limpieza por chunks
â”‚
â”œâ”€â”€ ğŸ“ Transform/                 # ğŸ”„ TransformaciÃ³n
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ Transform.py             # Clase DataTransformer (15+ funciones)
â”‚
â”œâ”€â”€ ğŸ“ Load/                      # ğŸ’¾ Persistencia
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ Load.py                  # Clase DataLoader (4 formatos)
â”‚
â”œâ”€â”€ ğŸ“ Vizualize/                 # ğŸ“Š VisualizaciÃ³n
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ plot.py                  # 11 funciones de grÃ¡ficas
â”‚
â”œâ”€â”€ ğŸ“ Output/                    # ğŸ“‚ Archivos de salida
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ IntegratedData_cleaned.csv
â”‚   â”œâ”€â”€ IntegratedData_transformed.csv
â”‚   â”œâ”€â”€ agregado_nacional.csv
â”‚   â”œâ”€â”€ top_estados.csv
â”‚   â”œâ”€â”€ top_condados.csv
â”‚   â””â”€â”€ ğŸ“ figures/              # 11 visualizaciones PNG
â”‚       â”œâ”€â”€ 1_evolucion_casos_muertes.png
â”‚       â”œâ”€â”€ 2_top_condados_casos.png
â”‚       â”œâ”€â”€ 3_casos_vs_muertes.png
â”‚       â”œâ”€â”€ 4_movilidad_correlacion.png
â”‚       â”œâ”€â”€ 5_comparacion_dias.png
â”‚       â”œâ”€â”€ 6_top_estados_casos.png
â”‚       â”œâ”€â”€ 7_tasa_mortalidad_estados.png
â”‚       â”œâ”€â”€ 8_evolucion_movilidad.png
â”‚       â”œâ”€â”€ 9_casos_dia_semana.png
â”‚       â”œâ”€â”€ 10_promedio_movil.png
â”‚       â””â”€â”€ 11_mapa_calor_correlacion.png
â”‚
â”œâ”€â”€ pipeline.py                  # ğŸš€ Pipeline ETL completo
â”œâ”€â”€ IntegratedData.csv           # ğŸ“Š Dataset original (77MB)
â”œâ”€â”€ requirements.txt             # ğŸ“¦ Dependencias Python
â”œâ”€â”€ README.md                    # ğŸ“– Esta documentaciÃ³n (tÃ©cnica)
â””â”€â”€ DATASET_INFO.md              # ğŸ“Š InformaciÃ³n del dataset (no tÃ©cnica)
```

---

## ğŸ› ï¸ TecnologÃ­as Utilizadas

- **Python 3.x** - Lenguaje de programaciÃ³n
- **Pandas** - ManipulaciÃ³n de datos (lectura por chunks, limpieza)
- **NumPy** - Operaciones numÃ©ricas
- **Matplotlib** - Visualizaciones base
- **Seaborn** - GrÃ¡ficas estadÃ­sticas avanzadas
- **Git/GitHub** - Control de versiones

---

## ğŸ“Š Estado Actual del Proyecto

- âœ… **ConfiguraciÃ³n:** MÃ³dulo completo con todas las constantes
- âœ… **ExtracciÃ³n:** 7 mÃ©todos diferentes de lectura
- âœ… **Limpieza:** Procesamiento por chunks implementado
- âœ… **TransformaciÃ³n:** 15+ funciones de anÃ¡lisis
- âœ… **Carga:** Soporte para 4 formatos
- âœ… **VisualizaciÃ³n:** 11 grÃ¡ficas profesionales
- âœ… **Pipeline:** Script integrador funcional
- âœ… **DocumentaciÃ³n:** README tÃ©cnico completo

---

## ğŸš¦ CÃ³mo Empezar a Desarrollar

### 1. Clonar y configurar entorno

```bash
git clone https://github.com/kenmaroyert1/WilsonTrabajo1.git
cd WilsonTrabajo1
pip install -r requirements.txt
```

### 2. Familiarizarse con la configuraciÃ³n

```python
from Config.Config import *

# Ver rutas configuradas
print(f"Root: {PROJECT_ROOT}")
print(f"Data: {DATA_DIR}")
print(f"Output: {OUTPUT_DIR}")

# Ver parÃ¡metros
print(f"Chunk size: {CHUNK_SIZE}")
```

### 3. Probar mÃ³dulos individuales

```python
# ExtracciÃ³n
from Extract.Extract import DataExtractor
extractor = DataExtractor("IntegratedData.csv")
df_sample = extractor.extract_sample(frac=0.01)  # 1% de datos

# Limpieza
from Extract.Clean.Clean import clean_csv
clean_csv("test_input.csv", "test_output.csv")

# TransformaciÃ³n
from Transform.Transform import DataTransformer
transformer = DataTransformer()
df = transformer.add_moving_average(df, 'cases', window=7)

# VisualizaciÃ³n
from Vizualize.plot import crear_serie_temporal_casos
crear_serie_temporal_casos("Output/IntegratedData_transformed.csv")
```

### 4. Ejecutar pipeline completo

```bash
python pipeline.py
```

### 5. Verificar resultados

- Revisar `Output/IntegratedData_cleaned.csv`
- Revisar `Output/IntegratedData_transformed.csv`
- Ver grÃ¡ficas en `Output/figures/`

---

## ğŸ§ª Testing y Debugging

### Probar con datos pequeÃ±os

```python
# Usar muestra del 10%
extractor = DataExtractor("IntegratedData.csv")
df_test = extractor.extract_sample(frac=0.1)
df_test.to_csv("test_sample.csv", index=False)

# Ejecutar pipeline con muestra
python pipeline.py --input test_sample.csv
```

### Verificar chunks

```python
from Extract.Extract import DataExtractor

extractor = DataExtractor("IntegratedData.csv")
for i, chunk in enumerate(extractor.extract_chunks(chunk_size=10000)):
    print(f"Chunk {i}: {len(chunk)} filas, {chunk.memory_usage().sum() / 1024**2:.2f} MB")
    if i >= 5:  # Solo primeros 5 chunks
        break
```

### Validar transformaciones

```python
from Transform.Transform import DataTransformer
import pandas as pd

df = pd.read_csv("Output/IntegratedData_cleaned.csv", nrows=1000)
transformer = DataTransformer()

# Antes
print("Antes:", df.columns.tolist())
print("Shape:", df.shape)

# Transformar
df = transformer.add_temporal_features(df)

# DespuÃ©s
print("DespuÃ©s:", df.columns.tolist())
print("Shape:", df.shape)
```

---

## ğŸ“– DocumentaciÃ³n Relacionada

- **[DATASET_INFO.md](DATASET_INFO.md)** - InformaciÃ³n completa sobre el dataset, visualizaciones, casos de uso e interpretaciÃ³n de grÃ¡ficas

---

## ğŸ‘¥ Contribuciones

Este es un proyecto acadÃ©mico. Para consultas o sugerencias, contactar al equipo de desarrollo.

---

## ğŸ“ Licencia

Proyecto acadÃ©mico - Universidad

---

## ğŸ“ Contacto

- **Repositorio:** https://github.com/kenmaroyert1/WilsonTrabajo1
- **Autor:** Wilson
- **Curso:** AnÃ¡lisis de Datos / Ciencia de Datos
